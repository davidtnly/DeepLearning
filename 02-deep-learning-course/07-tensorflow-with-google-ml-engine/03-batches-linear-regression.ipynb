{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern data sets can contain millions or even billions of data points. So training one value at a time is completely impractical. For this reason modern applications split their input data into batches. \n",
    "\n",
    "A __batch__ is a set of data points that are trained in a single training step. Batch size plays an important role in determining an applications performance and accuracy. \n",
    "\n",
    "### Small vs. Large\n",
    "If batches are small an application may take a long time to train. If batches are large the training time may decrease, but the applications accuracy may also decrease. Unfortunately, there's no clear rule for selecting batch size. \n",
    "\n",
    "Split the training data into subsets called batches. So each trainining step processes one batch and the batch size is determined by trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomizing / Shuffling Training Batches; To Solve - Stuck at Local Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training methods frequently get stuck at local minima instead of finding the global point of minimum loss. One way to prevent this from happening is to randomize or shuffle the training batches. \n",
    "\n",
    "Batch shuffling increases the likelihood of finding global minimum.\n",
    "\n",
    "If you shuffle the batches in the gradient decent method the resulting algorithm is called the __Stochastic Gradient Descent__, or SGD algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders (Tensors that receive different batches of data)\n",
    "\n",
    "If a tensor contains an input batch of data it will need to updated with each training step.  (check tensorflow folder for more examples)\n",
    " \n",
    "This process of updating the input tensor is called __feeding__ and TensorFlow provides a specific type of tensor for receiving batch data. This new type of tensor is called a __placeholder (type of tensor)__, and you can use a placeholder by calling __tf.placeholder__.\n",
    "\n",
    "    tf.placeholder(dtype, shape=None) # no initial values until fed data\n",
    "\n",
    "This function requires the data type of the input data and an optional parameter sets the batches shape. Tf.placeholder returns a tensor without any values. Its values will be provided as the training is performed. \n",
    "\n",
    "We can feed data into a placeholder using the second parameter of the sessions run method. This is called __feed_dict__ and it accepts a dictionary that associates placeholder names with data. This simple example code demonstrates how batches and placeholders work together.\n",
    "\n",
    "    sess.run(feed_dict) # dictionary that associates placeholders with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 10\n",
    "\n",
    "# Configured to contain 1,000 floats in a 100 x 10 matrix\n",
    "holder = tf.placeholder(tf.float32, shape=[100,10]) # returns a tensor named holder\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(num_batches):\n",
    "        batch_data = np.array(...)\n",
    "        \n",
    "        '''\n",
    "         When you feed data into a placeholder with feed_dict you can't provide the \n",
    "         data as a tensor. But you can provide the data as a numpy array. This is important \n",
    "         to keep in mind when you use batches and placeholders. \n",
    "        '''\n",
    "        \n",
    "        sess.run(op, feed_dict={holder: batch_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Example\n",
    "\n",
    "- Set constants\n",
    "\n",
    "\n",
    "- Generate input points\n",
    "\n",
    "\n",
    "- Create variables and placeholders\n",
    "\n",
    "\n",
    "- Define model and loss\n",
    "\n",
    "\n",
    "- Create optimizer\n",
    "\n",
    "\n",
    "- Execute optimizer in a session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m -  0.49117818\n",
      "b -  0.977285\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set constants\n",
    "N = 1000\n",
    "learn_rate = 0.1\n",
    "batch_size = 40\n",
    "num_batches = 400\n",
    "\n",
    "# Step 1: Generate input points\n",
    "x = np.random.normal(size=N) # get values to surround the line Y equals 0.5, X plus one\n",
    "m_real = np.random.normal(loc=0.5, scale=0.2, size=N) # generate points with a mean of 0.5, and sd to 0.2\n",
    "b_real = np.random.normal(loc=1.0, scale=0.2, size=N) # formula is y = 0.50X + 1, so 1 = b\n",
    "y = m_real * x + b_real\n",
    "\n",
    "# Step 2: Create variables and placeholders\n",
    "m = tf.Variable(tf.random_normal([])) # set initial value with a generic shape\n",
    "b = tf.Variable(tf.random_normal([]))\n",
    "x_holder = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "y_holder = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "# Step 3: Define model and loss\n",
    "model = m * x_holder + b # general equation of the line\n",
    "\n",
    "'''\n",
    "Computing the average of a tensor using tf.reduce_mean. For the argument, I'll use tf.pow, \n",
    "model minus y_holder and then two. And what this will do is it will compute model minus y_holder \n",
    "for each training step and square that\n",
    "'''\n",
    "\n",
    "loss = tf.reduce_mean(tf.pow(model-y_holder, 2)) # power of 2 (mean_squared_error, no negatives)\n",
    "\n",
    "# Step 4: Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "# Step 5: Execute optimizer in a session\n",
    "with tf.Session() as sess:\n",
    "    '''Execute the operation that initializes both of the variables in the application by calling sess.run'''\n",
    "    sess.run(tf.global_variables_initializer()) # initialize global variables\n",
    "\n",
    "    # Perform training in a loop that executes num_batches times\n",
    "    for _ in range(num_batches):\n",
    "\n",
    "        '''Within each step, I need to generate the data that's going to be sent into the two placeholders'''\n",
    "        x_data = np.empty(batch_size) # create empty numpy array\n",
    "        y_data = np.empty(batch_size)\n",
    "        \n",
    "        '''Generate values for that particular batch by using another loop'''\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, N) # generate an index value into the original data batch\n",
    "            x_data[i] = x[index] # set ith value of the batch equal to X index\n",
    "            y_data[i] = y[index]# set ith value of the batch equal to Y index\n",
    "\n",
    "        '''Start the training process by calling sess.run() with the optimizer'''\n",
    "        sess.run(optimizer, feed_dict={x_holder: x_data, y_holder: y_data})\n",
    "\n",
    "    print('m - ', sess.run(m))\n",
    "    print('b - ', sess.run(b))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrated, in practice, how variables, training, and optimizers work together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
