{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pre-trained Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition Models Pre-Trained in Keras\n",
    "\n",
    "1. VGG (University of Oxford)\n",
    "2. ResNet50 (Microsoft)\n",
    "3. Inception v3 (Google)\n",
    "4. MobileNet (Google)\n",
    "5. NASNet (Google)\n",
    "\n",
    "All of the pre-trained models included are under the applications package. \n",
    "\n",
    "    keras.applications import *name*\n",
    "\n",
    "### Transfer Learning\n",
    "Adapt an existing model to recognize new types of objects instead of starting from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Image Recognition Pre-trained Model\n",
    "\n",
    "Use a VGG model to classify an image. The bay.jpg image that will be loaded is too large to process directly with the neural network. When you feed images into a neural network, the size of the image need to match the number of input nodes in the neural network.\n",
    "\n",
    "For VGG, images we need into the network need to be 224x224 pixels. Set target_size parameter to (224, 224)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Keras' VGG16 model that was pre-trained against the ImageNet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image file and resize it to 224x224 (required by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('bay.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the image to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a fourth dimension (Keras expects a list but we only have one image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the input image's pixel values to the range used when training the neural network\n",
    "\n",
    "When we feed images into the neural network, they always need to be normalized so that the values for each pixel are between zero and one. The vgg model has a built in normalization function called pre-process input that will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vgg16.preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the image through the deep neural network to make a prediction\n",
    "\n",
    "The predictions we get back will be the 1,000 element array of floating point numbers. Each element in the array will tell us how likely our picture contains each of the 1,000 objects the model was training to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up the names of the predicted classes. Index zero = result for the first image\n",
    "\n",
    "The vgg model provides a decode predictions function that will tell us the names of the most likely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = vgg16.decode_predictions(predictions, top=9)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results and likelihood of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top predictions for this image:')\n",
    "\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print('Predictions: {} - {:2f}'.format(name, likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Using a model trained on one set of data as a starting point for modeling a new set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lot of training data, you can train a convolutional neural network to recognize objects and images, but here's a secret, in the real world, you almost never need to train the neural network from scratch. Instead, we can use transfer learning to reuse an existing neural network and adapt it to a new problem. \n",
    "\n",
    "### From One Learned Model to Another\n",
    "Transfer learning is where you take a model trained on one set of data and then use the knowledge it learned to give it a headstart when solving a new problem. To understand how transfer learning works, let's take a look at how a convolutional neural network processes an image layer by layer. A typical convolutional neural network is structured like this. The network is made up of a series of convolutional layers and the training process teaches each of those layers to be activated when it sees certain patterns in the input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert picture here cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those layers learn to tell images apart by looking for those unique patterns. Let's look at the patterns that would activate each layer. These are the kinds of patterns that the first layer is looking for. You can see that it's looking for really basic patterns like splotches of color and lines in an image. \n",
    "\n",
    "Now let's go down to the second convolutional layer. The patterns are starting to look slightly more complex. Here are the images that would activate the third layer. These patterns are starting to resemble objects. The one on the right sort of looks like a hair pattern. Here are some images that would activate the fourth layer. The patterns are even more complex. The middle image almost looks like eyeballs. Let's look at one more. Here are the images that would activate the fifth layer. Now the patterns are getting very complex with curves and shapes. \n",
    "\n",
    "Since neural networks are really just mathematical models, it's impossible to tell exactly what each of these patterns represents, but you can see how each layer is getting more and more complex in what it's looking for. \n",
    "\n",
    "### Basic Idea of a Neural Network\n",
    "The basic idea is that neural networks learn to detect simple patterns in the top layer, and then the next layer uses that information to look for slightly more complex patterns and so on, down through all the convolutional layers. \n",
    "\n",
    "But the final layer of the neural network is a densely connected layer that uses the information from the convolutional layers to decide which object is in the image. With transfer learning, we're gonna start with a neural network that's already been trained to recognize objects from a large dataset like ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert picture here of dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse this neural network with new data, we can simply slice off the last layer. We'll keep all the layers that detect patterns, but remove the part that maps those patterns to specific objects. We'll call this pre-trained neural network a feature extractor because we're using it to extract training features from images. Next, we'll create a new neural network to replace the last layer in the original network. This is the only part that we'll have to train ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert new neural network image here (dense layer pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build our new image recognition system, we'll pass our new training images through the feature extractor and save the results for each training image to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert pic here of training with transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll use those extracted features to train the new neural network. Since we're using the feature extractor to recognize shapes and patterns, our new neural network only has to learn to tell which patterns map to which objects. Since this new neural network isn't doing much work, it can learn to do it with a small amount of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert pic of the saved training features dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's how we'll do predictions with transfer learning. When we want to test the new image, we have to first pass it through the same feature extractor. Then we can use those extracted features as input to our newly-trained neural network, which will give us the final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert pic with new network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Transfer Learning\n",
    "- Transfer learning can cut the time required to build an image recognition system from days to minutes\n",
    "- When I'm starting new projects, I almost always try transfer learning first before training a new neural network from scratch\n",
    "- If transfer learning works, there's no need to do the extra work to train a new model\n",
    "- Transfer learning is also really useful when you only have a small training dataset available.\n",
    "\n",
    "Training a neural network from scratch is sort of like teaching a baby to read. The baby has to learn about letters and words and sentences before it can read and understand anything. Transfer learning is more like asking an adult that already knows how to read to learn something new. Since the adult already knows how to read, they need less material to learn a new topic. They don't need alphabet flashcards and spelling tests. The same basic idea applies to neural networks. \n",
    "\n",
    "### Small Dataset? Use Transfer Learning\n",
    "If you only have a few hundred training images for your image recognition system, you don't have enough data to teach your model from scratch, so it makes sense to start with a model trained for something else and adapt it to your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features w/ a Pre-Trained Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk through directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use transfer learning to build an image recognition system that can identify pictures of dogs. \n",
    "\n",
    "[Relative Path](https://kite.com/python/examples/4293/os-get-the-relative-paths-of-all-files-and-subdirectories-in-a-directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training folder\n",
    "training_folder = 'TrainingData/'\n",
    "subfolder_dogs = '/dogs/'\n",
    "subfolder_not_dogs = '/not_dogs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['00001.png', '00005.png', '00087.png', '00120.png', '00132.png', '00148.png', '00150.png', '00153.png', '00154.png', '00155.png', '00267.png', '00287.png', '00313.png', '00316.png', '00350.png', '00355.png', '00362.png', '00382.png', '00393.png', '00400.png', '00401.png', '00405.png', '00410.png', '00446.png', '00448.png', '00450.png']\n",
      "['00381.png', '00383.png', '00384.png', '00385.png', '00386.png', '00387.png', '00390.png', '00395.png', '00397.png', '00399.png', '00413.png', '00414.png', '00415.png', '00417.png', '00418.png', '00422.png', '00426.png', '00427.png', '00428.png', '00434.png', '00438.png', '00439.png', '00441.png', '00449.png', '00454.png', '00459.png', '00470.png', '00471.png', '00474.png', '00477.png', '00479.png', '00480.png']\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(training_folder):\n",
    "    print(files)\n",
    "#     for d in dirs:\n",
    "#         print(d)\n",
    "#         print(os.path.relpath(os.path.join(root, d), \".\"))\n",
    "#     for f in files:\n",
    "#         print(os.path.relpath(os.path.join(root, f), \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.relpath(os.path.join(root, d), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Feature Extractor\n",
    "The first step is to build a feature extractor that can extract training features from our images.\n",
    "\n",
    "1. Load libraries, images, training data\n",
    "2. Build a VGG model without the top layer (include_top=False)\n",
    "3. Predict training data\n",
    "4. Extract features and labels using joblib into a dat file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I have a sub-folder called dogs. These pictures are 64 by 64 pixel images from the image net dataset. If you're building your own image recognition system, you can use your own pictures of whatever kind of objects you wanna recognize instead. Next, we have a folder called \"not dogs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "\n",
    "from PIL import Image\n",
    "# potato_resize = Image.open('Images/23-potato.png').resize((64,64))\n",
    "# potato_resize.save('Images/23-potato-resize.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create path to folders with training data\n",
    "dog_path = Path(training_folder) / subfolder_dogs\n",
    "not_dog_path = Path(training_folder) / subfolder_not_dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty arrays to store image array and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an images and labels list to store arrays; assign 0 or 1 as labels\n",
    "images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through not dog path folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through not-dog images (not_dog_path)\n",
    "for img in not_dog_path.glob('*.png'): # return a possibly-empty list of path names that match pathname \"*.png\"\n",
    "    \n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img) # keras helper function to convert image into an array\n",
    "    \n",
    "    # Add the image to the lsit of images\n",
    "    images.append(image_array) # add image appray to list of images\n",
    "    \n",
    "    # Add labels which is 0 for not dogs\n",
    "    labels.append(0) # add 0 to label array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through dog path folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dog images\n",
    "for img in dog_path.glob('*.png'):\n",
    "    \n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img)\n",
    "    \n",
    "    # Add the image to the list of images\n",
    "    images.append(image_array)\n",
    "    \n",
    "    # For each dog image, the expected value should be 1\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data array (x & y)\n",
    "Create an array called x_train that will have the training data. Keras expects all of our training images to be a numpy array instead of a normal Python list.\n",
    "\n",
    "1. Create a single numpy array with all the images laoded\n",
    "2. Convert the labels to a numpy array\n",
    "3. Normalize from 0 to 1\n",
    "4. Pre-trained neural network\n",
    "5. Extract features for each image (all in one pass)\n",
    "6. Save the array of extracted features to a file\n",
    "7. Save the matching array of expected values to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Path.glob of WindowsPath('/not_dogs')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a single numpy array with all the images\n",
    "x_train = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array as well\n",
    "y_train = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize training data\n",
    "\n",
    "Need to normalize our training dataset so all the pixel values are in the zero to one range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vgg16.preprocess_input(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-trained neural network to use as a feature extractor\n",
    "\n",
    "Create a VGG16 object (pre-trained). Our training images are 64 pixels by 64 pixels with three color channels, one for red, one for green, and one for blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained neural network to extract features; remove last layer of the neural network\n",
    "pretrained_nn = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for each image (all in one pass)\n",
    "\n",
    "Features x array will now contain the set of features that represent each of the training images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each image\n",
    "features_x = pretrained_nn.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the array of extracted features to a file\n",
    "\n",
    "Save the features to disk. Use dump() from joblib for writing an array to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features array\n",
    "joblib.dump(features_x, 'x_train.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the matching array of expected values to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save expected valuess array\n",
    "joblib.dump(y_train, 'y_train.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the features and labels that represent our training data. \n",
    "\n",
    "## Extraction Complete End Here Before New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a New Neural Network w/ Extracted Features (Load)\n",
    "We've used the pre trained neural network to extract features from our training images. Now we're ready to train a new neural network that uses those extracted features.\n",
    "\n",
    "### Differences\n",
    "The code is exactly like training any other neural network but with two small differences. \n",
    "1. The first difference is how we load our training data Instead of loading raw images to train with, we're gonna load the features that we extracted with the pre trained VGG 16 neural network. If you look at the file list on the left, you can see that we already have our extracted features stored in a file called x train.dat and our labels stored in a file called y train.dat.\n",
    "\n",
    "2. The second difference is in how we define our layers. Since we use VGG 16 to extract features from our image, this neural network has no convolutional layers. Instead it only has the final dense layers of the neural network. These are the only layers that we'll be retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "x_train = joblib.load('x_train.dat')\n",
    "y_train = joblb.load('y_train.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model and add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network structure\n",
    "transfer_model_structure = model.to_json()\n",
    "f = Path('transfer_model_structure.json')\n",
    "f.write_text(transfer_model_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save neural network's trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network's trained weights\n",
    "model.save_weights('transfer_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions with Transfer Learning\n",
    "\n",
    "We have used the pre-trained models, extracted features, created a new model that will train the dense layers, extracted the structure and trained weights using the combination of the two. Now we will make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is exactly the same as the code we'd use to make predictions with the standard neural network. There's just one key change. We'll need the pre-processor image with the vgg16 feature extractor. \n",
    "\n",
    "First, we can see that we're loading the structure of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.applications import vgg16\n",
    "\n",
    "# Load the json file that contains the transfer model structure\n",
    "f = Path('transfer_model_structure.json')\n",
    "model_structure = f.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json daa\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "# Re-load the transfer model's trained weights\n",
    "model.load_weights('transfer_model_weights.h5')\n",
    "\n",
    "# Load an image file to test and resize it to 64x64\n",
    "img = image.load_img('Images/23-potato.png', target_size=(64, 64))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_array = image.img_to_array(img)\n",
    "\n",
    "# Add a forth dimension to the image\n",
    "images = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Normalize the data\n",
    "images = vgg16.preprocess_input(images)\n",
    "\n",
    "# Use the pre-trained neural network to extract features from the test image\n",
    "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "features = feature_extraction_model.predict(images)\n",
    "\n",
    "# Given the extracted features, make a final prediction using the model\n",
    "results = model.predict(features)\n",
    "\n",
    "# Check first result since we only have one image\n",
    "single_result = results[0][0]\n",
    "\n",
    "# Print the result\n",
    "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))\n",
    "\n",
    "Image.open('Images/23-potato.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
