{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1\n",
    "\n",
    "Learning Objectives:\n",
    "- What's Keras?\n",
    "- Using Keras vs. TensorFlow\n",
    "- Training a deep learning model\n",
    "- Using a pre-trained deep learning model\n",
    "- Monitoring a Keras model with TensorBoard\n",
    "- Using a trained Keras model in Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Keras?\n",
    "\n",
    "Keras is a popular programming framework for deep learning that simplifies the process of building deep learning applications (deep neural networks). \n",
    "\n",
    "It's a frontend layer that uses python and uses either TensorFlow or Theano behind the scenes and adds a standard, simplified programming interface on top. It abstracts away a lot of the complexity of using those tools while still giving you many of the benefits.\n",
    "\n",
    "When you use Keras with Tensorflow, it builds a Tf model and runs the training process for you. That means your model is compatible with most tools and utilities that work with Tf.\n",
    "\n",
    "#### What makes Keras unique? \n",
    "Industry best practices are built-in. When building a deep learning system there are many different parameters you have to configure. Keras always tried to provide good defaults for parameters. Keras also comes with several pre-trained deep learning models for image recognition.\n",
    "\n",
    "    Built-In Image Recognition Model - Image File -> Pre-Trained Model -> 'seashore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Backends\n",
    "\n",
    "Keras is a high level tool for coding and training neural networks. You can think of it as a set of building blocks that you can combine to create neural networks, but Keras is just the front-end layer. It doesn't do all the processing on its own. \n",
    "\n",
    "Instead, it utilizes a separate deep-learning library under the hood for the processing. But what makes Keras especially unique is that it isn't limited to using just one deep-learning library. Keras currently lets you choose between Google's TensorFlow or the University of Montreal's Theano as the library to power your neural networks. Each has its own advantages and both are very capable and popular choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras vs. TensorFlow\n",
    "\n",
    "We'll be using Keras with the TensorFlow backend. That means we'll write our code with Keras, but the actual processing will be done with TensorFlow. \n",
    "\n",
    "#### So why use Keras?\n",
    "So why are we going through the extra step of using Keras instead of just using TensorFlow on its own. TensorFlow is a popular tool for building and training deep neural networks. It's used by many companies and research institutions everyday to build cutting edge systems. However, TensorFlow is a low-level tool. It's designed to give you total control over the design of your neural network, but it makes you do a lot of the work on your own. Code written with TensorFlow tends to be long and detailed. Even defining and training a basic neural network can take several pages of code. Keras, on the other hand, is a high-level solution. Its primary design goal is fast and easy experimentation. The idea is that if you spend less time coding, you can spend more time experimenting. \n",
    "\n",
    "Here is an example of defining the same neural network with TensorFlow and then with Keras. Both of these code samples do the same thing. They define the neural network that has ten inputs, several layers, and then a single output. But even without understanding how the code works, you can see that the TensorFlow version is longer and more detailed than the Keras version. That's because TensorFlow gives you more control over almost every detail. If you want to add a new layer to your neural network, you have to explicitly tell it how to do all the math that takes place inside that layer. Keras works at the much higher level of detail. \n",
    "\n",
    "To add a new layer to a neural network in Keras, you just call the _model.add_ function and tell it what kind of layer you want. That's it. It's much simpler. But it also means that you're limited to using the types of layers that Keras has built-in. \n",
    "\n",
    "#### When is using TensorFlow better?\n",
    "So when is using TensorFlow alone a better choice? \n",
    "- TensorFlow is a great choice if you are exploring brand new approaches to machine learning. In that case, you need the ability to tweak every detail of your machine learning model. The canned approach provided by Keras won't give you enough flexibility. \n",
    "- Using TensorFlow directly is also a great choice if you're building a giant machine learning system that will support many users. In that case, saving a little time upfront on writing the code might not be worth it in the end. You'll want to have control over every detail of how the system works. \n",
    "- In general, TensorFlow is a good choice when control over how memory and processing power are used are more important than any coding time you'd save using Keras. \n",
    "\n",
    "#### When is Keras a good choice? \n",
    "- First, Keras is a great educational tool. Since it lets you quickly try out the most popular types of neural networks, it's a great way to experiment with deep learning without spending a lot of time having to learn the ins and outs of a tool like TensorFlow. \n",
    "- Keras is also great for prototyping new machine learning systems. Because it's so much faster to code with Keras, you can try out lots of different ideas in a small amount of time. So even if you will ultimately build your production system with TensorFlow, Keras is a great tool to use to validate the basic design. \n",
    "- But Keras isn't limited to just education and prototyping, Keras is also used for production systems and works well in many cases. \n",
    "- So unless you have highly specialized needs or are building a large system for millions of users, it's worth considering using Keras. \n",
    "\n",
    "#### Summary\n",
    "Tensorflow\n",
    "- Low level\n",
    "- More control\n",
    "- Write more code\n",
    "\n",
    "Keras\n",
    "- High Level\n",
    "- Fast Experimentation\n",
    "- Write less code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Keras\n",
    "\n",
    "Installing Keras into Conda environment\n",
    "\n",
    "    conda install -c conda-forge keras # Installs keras (could do this after creating keras env and activation)\n",
    "    conda create -n keras keras # Create conda environment named keras\n",
    "    conda activate keras\n",
    "    conda deactivate\n",
    "\n",
    "#### Tensorflow\n",
    "[Anaconda TensorFlow Installation](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/)\n",
    "\n",
    "    conda create -n tf tensorflow\n",
    "    \n",
    "#### Google API\n",
    "\n",
    "    conda install -c conda-forge google-api-python-client (be sure to be in keras env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Conda Environment\n",
    "\n",
    "http://docs.anaconda.com/anaconda-cloud/user-guide/tasks/work-with-environments/\n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "https://stackoverflow.com/questions/55290271/updating-anaconda-fails-environment-not-writable-error\n",
    "\n",
    "https://stackoverflow.com/questions/56765518/anaconda-4-7-5-warning-about-conda-build-3-18-3-and-issues-with-python-packag\n",
    "\n",
    "1. Following the first video on installation - install anaconda and conda create -n keras keras, then go into the environment and change python version to 3.6 to get tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras with Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Neural Network in Keras (Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we'll use Keras to build and train a supervised machine learning model. Supervised machine learning is the branch of machine learning where we train the model by showing it input data and the expected result for that data, and it works out how to transform the input into the expected output. \n",
    "\n",
    "When building a supervised machine learning model, there's a process we follow, called the model train test evaluation flow.\n",
    "1. Choose ML Algorithm to use\n",
    "    - First, we need to choose which machine learning algorithm we want to use. We can pick any standard machine learning algorithm, but with Keras, you'll always be using neural networks. \n",
    "2. Training Phase\n",
    "    - Then we start the training phase. We train the algorithm by showing it training data and the expected output for that data, and it has to figure out how to replicate the expected result. For example, if we show it the numbers two and two, and tell it the result should be four. And then we show it three and five, and tell it the result is eight. It will work out that the inputs should be added together to get the desired output. \n",
    "3. Testing Phase\n",
    "    - After we train the model, we enter the testing phase. We load up a second set of data it has never seen before, called the testing data set, and then we feed this data through the model and make sure it is able to predict the correct result even though it has never seen this data before. This will show that the model actually learned how to solve the problem in a general way and didn't just memorize the answers for the training data. \n",
    "4. Evaluation Phase\n",
    "    - Finally, once the model is trained and tested, we can use it in the real world. This is the evaluation phase. We pass in new data, and it gives us a prediction. Keras makes it easy to set up a train, test, evaluation flow. \n",
    "5. Create a model\n",
    "    - First, we will create our neural network model. In Keras, we do that by creating a new instance of a model object. The model object represents the neural network we are building. Once we have a model object, we can add layers to the neural network just by calling model.add and passing in the type of layer we want to add. \n",
    "6. Compiling the Model (Building the TF Model)\n",
    "    - The final step of defining a model is to compile it. That's when Keras actually builds a TensorFlow model for us behind the scenes. When we compile the model, we need to tell Keras two important things. \n",
    "        - First, we need to tell it how we want to measure the accuracy of each prediction made by the model during the training process. This is called the __loss function__. Keras lets us choose from several standard loss functions or define our own. \n",
    "        - Second, we need to tell Keras which __optimizer algorithm__ we want to use to train the model. Keras lets us select from several popular optimizer algorithms. \n",
    "7. Train the Model with model.fit with Training Data        \n",
    "    - Now we're ready to start the training phase. To train the model, we call __model.fit__ and pass in the training data and the expected output for the training data. Keras will run the training process and print out the progress to the console. When training completes, it will report the final accuracy that was achieved with the training data. \n",
    "8. Test the Model with model.evaluate with Testing Data\n",
    "    - Once the model is trained, we're ready for the testing phase. We can test the model by calling __model.evaluate__ and passing in the testing data set and the expected output. \n",
    "9. Save the Model        \n",
    "    - When we are happy with the accuracy of the system, we can save the training model to a file. To do that, we call __model.save__ and pass in the file name. This file will contain everything we need to use our model in another program. \n",
    "10. Load the Model and Pass in New Data for Prediction    \n",
    "    - Now that we have a trained model, we're ready for the evaluation phase. We'll load our previously trained model by calling the load model function and passing in a file name. And then, to use the model to make new predictions, we just call the __predict__ function and pass in the new data we want predictions for. \n",
    "\n",
    "And that's the basic flow in using Keras. In the next several videos, we'll go through this flow in more detail and build a working neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neural Network in Keras (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model with Keras (Code) Keras Skeleton\n",
    "\n",
    "    model = keras.models.Sequential() # creating a new instance of a model object; represents neural network model\n",
    "    \n",
    "    model.add(keras.layers.Dense()) # add layers to the neural network and paying in the type of layer we want to add\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam') # compile model; this step is when Keras actually builds a TensorFlow model; model.compile(loss function, optimizer algorithm)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model built\n",
    "\n",
    "    model.fit(training_data, expected_output) # pass in training data and expected output (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model built & save\n",
    "\n",
    "    error_rate = model.evaluate(testing_data, expected_output) # test using evaluate\n",
    "    \n",
    "    model.save('trained_model.h5') # save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model\n",
    "\n",
    "    model = keras.models.load_model('trained_model.h5') # load_model\n",
    "    \n",
    "    predictions = model.predict(new_data) # make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "\n",
    "# Create a sequential model (neural network) by declaring a sequential model object\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Add a first dense layer with 50 nodes using the 9 features/inputs from the dataset\n",
    "model.add(keras.layers.Dense(50, input_dim=9, activation='relu'))\n",
    "\n",
    "# Add a second dense layer object\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Fit the model with training data\n",
    "model.fit(training_data, expected_output)\n",
    "\n",
    "# Evaluate\n",
    "error_rate = model.evaluate(testing_data, expected_output)\n",
    "\n",
    "# Save the model\n",
    "model.save('trained_model.h5')\n",
    "\n",
    "# Load if new instance\n",
    "model = keras.models.load_model('trained_model.h5')\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Sequential API - [Keras Sequential Documentation](https://keras.io/models/sequential/)\n",
    "\n",
    "    compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None) \n",
    "\n",
    "#### What is a neural network?\n",
    "Keras makes it very straightforward to code neural networks. A neural network is a machine-learning algorithm made up of individual nodes called __neurons__. These nodes, or neurons, are arranged into a series of groups called __layers__. \n",
    "\n",
    "__Nodes__ in each layer are connected to nodes in the following layer. Data flows from the input to the output along these connections. Each individual node is trained to perform a simple mathematical calculation and then feed its data to all the nodes it's connected to. \n",
    "\n",
    "When many trained nodes are connected and data flows through the entire network from start to finish, neural networks are able to model complex operations. When designing a neural network in Keras, we have to decide how many layers there should be, how many nodes should be in each layer and how the layers should be connected to each other. Bigger models with more layers and more nodes can model more complex operations, but if you make the model too big, it will be slow the train and is likely to overfit the data set. \n",
    "\n",
    "#### Sequential Model API\n",
    "The easiest way to build a neural network in Keras is to use the so-called __sequential model API__. It's called the sequential model API because you first create an empty model object, and then you add layers to it one after another in sequence. \n",
    "\n",
    "Here's an example. \n",
    "1. First, we create a new empty neural network by creating a new sequential object. \n",
    "2. Then, we can add as many layers as we want by calling model.add and passing in a new layer object. In this case, we are adding a new densely connected layer of 32 nodes to the neural network. A densely connected layer is one where every node is connected to every node in the previous layer, and since this is the very first layer in the neural network, we also have to tell it how many input nodes there are by passing in input dim=9. We can continue adding layers the same way. This line adds another layer with 128 densely connected nodes, and this line will add the final layer with one output node. It's that easy to define the neural network in Keras. \n",
    "    \n",
    "Keras is designed to make it quick to code the neural network, but it still tries to give you a large amount of control over the structure of each layer. Let's talk about the different ways we can customize a neural network layer. \n",
    "\n",
    "Before values flow from nodes in one layer to the next, they pass through an __activation function__. Keras lets us choose which activation function is used for each layer by passing in the name of the activation function we want to use. In this case, I've told it to use a __rectified linear unit__, or RELU, activation function. Keras supports all the standard activation functions in use today. It even includes lots of esoteric ones that aren't widely used outside the research. There's also lots of less commonly needed things that we can customize in each layer beyond the activation function, but one of the guiding principles of Keras is that it will do the best thing it can if you don't specify extra parameters. In other words, the default settings are modeled after what are considered best practices, so most of the time just choosing the number of nodes in a layer and choosing the activation function is good enough. So far we've talked about densely connected layers which are the most basic type of layer, but Keras also supports many different types of neural network layers. \n",
    "\n",
    "Let's look at two other major types of layers that Keras supports. \n",
    "1. First are convolutional layers. These are typically used to process images or spacial data. \n",
    "2. Next are recurrent layers. Recurrent layers are special layers that have a memory built into each neuron. These are used to process sequential data like words in a sentence where the previous data points are important to understanding the next data point. You can mix layers of different types in the same model as needed. \n",
    "    \n",
    "#### Model Building Steps\n",
    "The final step of defining a model is to compile it by calling model.compile. This builds out the model you've defined in the Tensorflow backend. When you compile a model, you have to pass in the optimizer algorithm and the loss function you want to use. The optimizer algorithm is the algorithm used to train your neural network. The loss function is how the training process measures how right or how wrong your neural network's predictions are. In this case, I've used the adam optimizer function which is a common and powerful optimizer, and the mean squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Sequential Model\n",
    "\n",
    "The easiest way to build a neural network in Keras is to use the so-called sequential model API. It's called the sequential model API because you first create an empty model object, and then you add layers to it one after another in sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new empty neural network by creating a new sequential object\n",
    "    \n",
    "    model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add layers (call model, add nodes, add output node)\n",
    "\n",
    "    model.add(Dense(32, input_dim=9)) # call model.add and pass in a new layer object (densely connected layer of 32 nodes)\n",
    "    \n",
    "    model.add(Dense(128)) # adds another layer with 128 densely connected nodes\n",
    "    \n",
    "    model.add(Dense(1)) # add in a final layer with one output node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding an activation function for nodes to pass through\n",
    "\n",
    "    model.add(Dense(128, activation='relu')) # pass an activation function called RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Types of Layers Supported (Conv, Rec)\n",
    "\n",
    "#### Convoluational Layers (typically used to process images or spacial data)\n",
    "\n",
    "    keras.layers.convolutional.Conv2D()\n",
    "\n",
    "#### Recurrent Layers (special layer thas has memory built into each neuron to process sequential data)\n",
    "\n",
    "    keras.layer.recurrent.LSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Bringing it all together - Keras Sequential Model API\n",
    "\n",
    "    model.keras.models.Sequential()\n",
    "    model.add(Dense(32, input_dim=9))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse') # optimizer is used to train the neural network, loss is a function used to measure how right or how wrong the neural network's predictions are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Training Data\n",
    "\n",
    "Predict total earnings of a new game based on sales data.\n",
    "\n",
    "Run exercise script and fill-in notes to make the code usable\n",
    "1. Since the features are not equally on the range same like total earnings and is_portable, we need to scale the data so  that each value is between zero and one. Neural networks work best when each feature is scaled to the same range.\n",
    "    - MinMaxScaler from sci-kit learn library\n",
    "2. Scale both Training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the training data\n",
    "train = pd.read_csv('Data/sales_data_training.csv')\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_csv('Data/sales_data_test.csv')\n",
    "\n",
    "# Scale from 0 to 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "scaled_train = scaler.fit_transform(train)\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "# Scaled values\n",
    "print('Note: total_earning values were scaled by multiplying by {:.10f} and adding {:.6f}'.format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "# Set the values to a variable\n",
    "scale_value = scaler.scale_[8]\n",
    "scale_min = scaler.min_[8]\n",
    "\n",
    "# Create dataframe for new scaled data\n",
    "scaled_training_df = pd.DataFrame(scaled_train, columns=train.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_test, columns=test.columns.values)\n",
    "\n",
    "# Save scaled data to CSV files\n",
    "scaled_training_df.to_csv('sales_data_training_scaled.csv', index=False)\n",
    "scaled_testing_df.to_csv('sales_data_test_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequentiall\n",
    "from keras.layers import *\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "\n",
    "# Split into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential model by declaring a sequential object\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "Add in first densely connected layer\n",
    "Call model.add and add in dense layer object; 50 nodes; on the first layer specify the number of model inputs\n",
    "\n",
    "Add relu for all the layers except the last layer, which will let us model more \n",
    "complex non-linear functions.\n",
    "''' \n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Output node; since pred value should be a single value so no relu function\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile with loss function and optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that have built the deep learning model, we want to train the neural network now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model\n",
    "1. Get dataset\n",
    "2. Scale dataset\n",
    "3. Train the neural network model\n",
    "4. Evaluate the model\n",
    "5. Test on new data (predictions)\n",
    "6. Saving and loading the models for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the training data\n",
    "train = pd.read_csv('Data/sales_data_training.csv')\n",
    "\n",
    "# Load the testing data\n",
    "test = pd.read_csv('Data/sales_data_test.csv')\n",
    "\n",
    "# Scale from 0 to 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "scaled_train = scaler.fit_transform(train)\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "# Scaled values\n",
    "print('Note: total_earning values were scaled by multiplying by {:.10f} and adding {:.6f}'.format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "# Set the values to a variable\n",
    "scale_value = scaler.scale_[8]\n",
    "scale_min = scaler.min_[8]\n",
    "\n",
    "# Create dataframe for new scaled data\n",
    "scaled_training_df = pd.DataFrame(scaled_train, columns=train.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_test, columns=test.columns.values)\n",
    "\n",
    "# Save scaled data to CSV files\n",
    "scaled_training_df.to_csv('sales_data_training_scaled.csv', index=False)\n",
    "scaled_testing_df.to_csv('sales_data_test_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "test = pd.read_csv('Data/sales_data_test_scaled.csv')\n",
    "\n",
    "# Split train data into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential nn (keras sequential API)\n",
    "model = Sequential()\n",
    "\n",
    "# Add in dense layers and input nodes\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "model.add(Dense(100, activation='relu')) # second layer\n",
    "model.add(Dense(50, activation='relu')) # third layer\n",
    "model.add(Dense(1, activation='linear')) # output layer, predicts 1 value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X,             # training features\n",
    "          y,             # expected output\n",
    "          epochs=50,     # training passes (epoch)\n",
    "          shuffle=True,  # works best when randomly shuffled\n",
    "          verbose=2)     # print more detailed info\n",
    "\n",
    "# Split test data into X and y input arrays\n",
    "X_test = test.drop('total_earnings', axis=1).values\n",
    "y_test = test[['total_earnings']].values\n",
    "\n",
    "# Measure the error rate of the testing data\n",
    "test_error_rate = model.evaluate(X_test, y_test, verbose=0) # pass in verbose 0\n",
    "\n",
    "# Print error rate\n",
    "print('The mean squared error (MSE) for the test data set is: {}'.format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making predictions\n",
    "\n",
    "Use a training model to make predictions for new data. Following the next steps of training, we can add in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data\n",
    "X_new = pd.read_csv('Data/proposed_new_product.csv').values\n",
    "\n",
    "# Make predictions with the neural network\n",
    "# Keras always assumes that we are going to ask for multiple predictions and multiple output values in each prediction\n",
    "# Keras model always returns predictions in a 2D array\n",
    "prediction = model.predict(X_new)\n",
    "\n",
    "# Get the first element of the first prediction\n",
    "first_prediction = prediction[0][0]\n",
    "\n",
    "# Reverse the scaling to get the actual number (dollars)\n",
    "# The constants are min and max of the dataset so we will use those numbers\n",
    "first_prediction = first_prediction + 0.1159\n",
    "first_prediction = first_prediction / 0.000003698\n",
    "\n",
    "# Print prediction *$265k\n",
    "print('Earnings Prediction for the First Proposed Product - ${}'.format(first_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk (structure of neural network and trained weights)\n",
    "model.save('trained_model.h5') # pass in file name and store it in a hdf5 format\n",
    "\n",
    "print('Model is saved to disk as trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data.\n",
    "\n",
    "It's a binary file format designed for storing Python array data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model.load_model('trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Models in Keras\n",
    "- VGG (Visual Geometry Group at the University of Oxford)\n",
    "- ResNet50 (Microsoft Research)\n",
    "- Inception-v3 (Google)\n",
    "- Xception (Francois Chollet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 Deep Neural Network\n",
    "\n",
    "Model included with Keras to recognize objects and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import resnet50\n",
    "\n",
    "# Load ResNet50 model object\n",
    "model = resnet50.ResNet50()\n",
    "\n",
    "# Load image file, resize it 224x224 pixels (required by model)\n",
    "img = image.load_img('Data/bay.jpg', target_size=(224, 224)) \n",
    "\n",
    "'''\n",
    "- Use image.load_img function with a tuple of 224x224 which will scale the image down to that size\n",
    "\n",
    "- Keeping the image size small helps limit the number of neurons you need in a neural network which \n",
    "    makes the models more practical to train.\n",
    "    \n",
    "- When you feed images into a neural network the size of the image needs to match the number of input \n",
    "    nodes in the neural network\n",
    "'''\n",
    "\n",
    "# Convert the image to a numpy array (array of plain numbers that we can feed into the nn)\n",
    "x = image.img_to_array(img) # change 3D image where width, height, color = 3D\n",
    "\n",
    "'''\n",
    "- The neural network expects us to pass in an array and multiple images at once but there's only one right now. \n",
    "    This can be fixed by adding a fourth dimension to the array by using NumPy's expand dims function. Basically,\n",
    "    we need to turn one image into an array of multiple images with just one element.\n",
    "'''\n",
    "\n",
    "# Add a fourth dimension since Keras expects a list of images\n",
    "x = np.expand_dims(x, axis=0) # first axis\n",
    "\n",
    "# Scale the input image to the range used in the trained network\n",
    "scaled_x = resnet50.preprocess_input(x) # normalized data\n",
    "\n",
    "# Run each image through the deep neural network to make a prediction\n",
    "predictions = model.predict(scaled_x) # pass in scaled data to return a predictions object\n",
    "\n",
    "''' \n",
    "- The predictions object is a 1,000 element array of floating point numbers.\n",
    "\n",
    "- Each element in the array tells us how likely our picture contains each of 1,000 objects\n",
    "    the model is trained to recognize.\n",
    "\n",
    "- To make things easier, the ResNet50 model provides a decode predictions function that will\n",
    "    tell us the name of the most likely matches instead of making us check all 1,000 possible entries.\n",
    "'''\n",
    "\n",
    "# Look up the names of the predicted classes. Index zero is the results for the predicted classes\n",
    "predicted_classes = resnet50.decode_predictions(predictions, top=9) # pass in predictions object; defaults to top 5\n",
    "\n",
    "# Loop through the results\n",
    "print('This is an image of:')\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print(' - {}: {:2f} likelihood'.format(name, likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring a Keras model with TensorBoard (TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorBoard?\n",
    "\n",
    "TensorBoard is a web-based tool that lets us visualize our model's structureand monitor its training.\n",
    "\n",
    "#### How do we use TensorBoard?\n",
    "\n",
    "To use TensorBoard we need our Keras model to write log files in the format that TensorBoard can read. TB uses the information in these log files to generate visualizations, let's add TB logging to our Keras model. We already have our model built out that is named 'trained_model.h5'. Using the same code, add in the new section.\n",
    "\n",
    "1. Create a TensorFlow logger object between defining the model and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TensorFlow object in the model built earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "test = pd.read_csv('Data/sales_data_test_scaled.csv')\n",
    "\n",
    "# Split train data into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential nn (keras sequential API)\n",
    "model = Sequential()\n",
    "\n",
    "# Add in dense layers and input nodes\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2')) # second layer\n",
    "model.add(Dense(50, activation='relu', name='layer_3')) # third layer\n",
    "model.add(Dense(1, activation='linear', name='output_layer')) # output layer, predicts 1 value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Create a TensorBoard logger object\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',    # pass it a log file parameter; logs = folder name\n",
    "    write_graph=True,  # log the structure of the model\n",
    "    histogram_freq=5   # statistics on layers; passes through the training data it will log statistics\n",
    ")\n",
    "\n",
    "# Train the model (tell model to use logger)\n",
    "model.fit(X,                 # training features\n",
    "          y,                 # expected output\n",
    "          epochs=50,         # training passes (epoch)\n",
    "          shuffle=True,      # works best when randomly shuffled\n",
    "          verbose=2,         # print more detailed info\n",
    "          callbacks=[logger] # a list of functions we want Keras to call in an array; can have multiple functions\n",
    ")\n",
    "\n",
    "# Split test data into X and y input arrays\n",
    "X_test = test.drop('total_earnings', axis=1).values\n",
    "y_test = test[['total_earnings']].values\n",
    "\n",
    "# Measure the error rate of the testing data\n",
    "test_error_rate = model.evaluate(X_test, y_test, verbose=0) # pass in verbose 0\n",
    "\n",
    "# Print error rate\n",
    "print('The mean squared error (MSE) for the test data set is: {}'.format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After running the model_logging code, we can open up a terminal on where the file is located\n",
    "2. Run the following code: \n",
    "    \n",
    "    - tensorboard --logdir=02-deep-learning-course/logs # this is the log folder location\n",
    "    \n",
    "\n",
    "3. Copy and paste the address into a web page to view TensorBoard's computational graph\n",
    "4. Examine the graphs\n",
    "    - You can see that the main input is connected to layer_1, layer_2, layer_3, and finally the output. If the layers have the same color then that means they have the same internal structure\n",
    "    - The output layer is different because it uses a different activation function\n",
    "    - Zooming into the small line you can see numbers which represents a __tensor__ or an array of data being passed between the layers\n",
    "        - The numbers here represent the size of the tensor or array\n",
    "        - The initial inputs for this neural network are nine values that get passed to the first layer\n",
    "        - The question mark is the __batch size__ which means that TensorFlow can process batches of data at once\n",
    "        - It's a question mark because you can change the batch size depending on how much data ou want to process at once\n",
    "    - We can also expand the node by clicking layer_1 and expand it\n",
    "        - Inside the node are actions being done insight of this specific layer of the neural network\n",
    "5. Tracing the path of data through the graph\n",
    "    - Click the output_layer and click on 'Trace inputs' on the left side\n",
    "        - This highlights the path data flows through to generate a prediction from the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Set run name for log folder\n",
    "RUN_NAME = 'run_1_with_50_nodes'\n",
    "RUN_NAME_2 = 'run_2_with_5_nodes'\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "test = pd.read_csv('Data/sales_data_test_scaled.csv')\n",
    "\n",
    "# Split train data into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential nn (keras sequential API)\n",
    "model = Sequential()\n",
    "\n",
    "# Add in dense layers and input nodes\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2')) # second layer\n",
    "model.add(Dense(50, activation='relu', name='layer_3')) # third layer\n",
    "model.add(Dense(1, activation='linear', name='output_layer')) # output layer, predicts 1 value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Create a TensorBoard logger object\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),  # pass it a log file parameter; logs = folder name\n",
    "    write_graph=True,                    # log the structure of the model\n",
    "    histogram_freq=5                     # statistics on layers; passes through the training data it will log statistics\n",
    ")\n",
    "\n",
    "# Train the model (tell model to use logger)\n",
    "model.fit(X,                 # training features\n",
    "          y,                 # expected output\n",
    "          epochs=50,         # training passes (epoch)\n",
    "          shuffle=True,      # works best when randomly shuffled\n",
    "          verbose=2,         # print more detailed info\n",
    "          callbacks=[logger] # a list of functions we want Keras to call in an array; can have multiple functions\n",
    ")\n",
    "\n",
    "# Split test data into X and y input arrays\n",
    "X_test = test.drop('total_earnings', axis=1).values\n",
    "y_test = test[['total_earnings']].values\n",
    "\n",
    "# Measure the error rate of the testing data\n",
    "test_error_rate = model.evaluate(X_test, y_test, verbose=0) # pass in verbose 0\n",
    "\n",
    "# Print error rate\n",
    "print('The mean squared error (MSE) for the test data set is: {}'.format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Trained Keras Model in Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you want to be able to scale it up in production to serve lots of users. What should you do? Since we're using Keras with TensorFlow backend, we can export our Keras model as a TensorFlow model and once we have a TensorFlow model we can upload that to Google Cloud ML service. Using the Google Cloud ML service, we can support as many users as we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting TensorFlow model\n",
    "\n",
    "#### Add this code after building the model from above and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SavedModelBuilder object\n",
    "model_builder = tf.saved_model.builder.SavedModelBuilder('exported_model') # pass in a folder name to save in\n",
    "\n",
    "# Declare model inputs and outputs \n",
    "# Keras makes it easy as it keeps track of i/o of the model so we just need to pass out the TensorFlow\n",
    "\n",
    "# Pass in model.input as the name of the input for Tf to use\n",
    "inputs = {\n",
    "    'input': tf.saved_model.utils.build_tensor_info(model.input) # name of the input\n",
    "}\n",
    "\n",
    "# Pass in the model_output\n",
    "outputs = {\n",
    "    'earnings': tf.saved_models.utils.build_tensor_info(model.output)\n",
    "}\n",
    "\n",
    "# Create a Tf signature def: a function declaration in the programming language\n",
    "# Tf looks for this to know how to run the prediction function of our model\n",
    "signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "\n",
    "# Save both the structure of our model and current weights\n",
    "# Pass in the reference to the current Keras session and assign the model a special\n",
    "#    tag to make sure Tf knows this model is meant for serving users\n",
    "# Pass in the signature def we just created\n",
    "model_builder.add_meta_graph_and_varibles(\n",
    "    K.get_session(),\n",
    "    tags=[tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save (see newly created folder)\n",
    "model_builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the new folder called exported_model. \n",
    "\n",
    "1. First, we have the saved_model, that pb file. This file contains the structure of our model in Google's protobuff format. \n",
    "\n",
    "2. There's also a variable sub-folder. This contains a checkpoint of the train weights from our neural network. \n",
    "\n",
    "Alright, this model's now ready to uploaded to the Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
