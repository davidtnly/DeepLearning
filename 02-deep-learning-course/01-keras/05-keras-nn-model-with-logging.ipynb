{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring a Keras model with TensorBoard (TB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorBoard?\n",
    "\n",
    "TensorBoard is a web-based tool that lets us visualize our model's structureand monitor its training.\n",
    "\n",
    "#### How do we use TensorBoard?\n",
    "\n",
    "To use TensorBoard we need our Keras model to write log files in the format that TensorBoard can read. TB uses the information in these log files to generate visualizations, let's add TB logging to our Keras model. We already have our model built out that is named 'trained_model.h5'. Using the same code, add in the new section.\n",
    "\n",
    "1. Create a TensorFlow logger object between defining the model and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TensorFlow object in the model built earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "test = pd.read_csv('Data/sales_data_test_scaled.csv')\n",
    "\n",
    "# Split train data into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential nn (keras sequential API)\n",
    "model = Sequential()\n",
    "\n",
    "# Add in dense layers and input nodes\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2')) # second layer\n",
    "model.add(Dense(50, activation='relu', name='layer_3')) # third layer\n",
    "model.add(Dense(1, activation='linear', name='output_layer')) # output layer, predicts 1 value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Create a TensorBoard logger object\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs',    # pass it a log file parameter; logs = folder name\n",
    "    write_graph=True,  # log the structure of the model\n",
    "    histogram_freq=5   # statistics on layers; passes through the training data it will log statistics\n",
    ")\n",
    "\n",
    "# Train the model (tell model to use logger)\n",
    "model.fit(X,                 # training features\n",
    "          y,                 # expected output\n",
    "          epochs=50,         # training passes (epoch)\n",
    "          shuffle=True,      # works best when randomly shuffled\n",
    "          verbose=2,         # print more detailed info\n",
    "          callbacks=[logger] # a list of functions we want Keras to call in an array; can have multiple functions\n",
    ")\n",
    "\n",
    "# Split test data into X and y input arrays\n",
    "X_test = test.drop('total_earnings', axis=1).values\n",
    "y_test = test[['total_earnings']].values\n",
    "\n",
    "# Measure the error rate of the testing data\n",
    "test_error_rate = model.evaluate(X_test, y_test, verbose=0) # pass in verbose 0\n",
    "\n",
    "# Print error rate\n",
    "print('The mean squared error (MSE) for the test data set is: {}'.format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After running the model_logging code, we can open up a terminal on where the file is located\n",
    "2. Run the following code: \n",
    "    \n",
    "    - tensorboard --logdir=02-deep-learning-course/logs # this is the log folder location\n",
    "    \n",
    "\n",
    "3. Copy and paste the address into a web page to view TensorBoard's computational graph\n",
    "4. Examine the graphs\n",
    "    - You can see that the main input is connected to layer_1, layer_2, layer_3, and finally the output. If the layers have the same color then that means they have the same internal structure\n",
    "    - The output layer is different because it uses a different activation function\n",
    "    - Zooming into the small line you can see numbers which represents a __tensor__ or an array of data being passed between the layers\n",
    "        - The numbers here represent the size of the tensor or array\n",
    "        - The initial inputs for this neural network are nine values that get passed to the first layer\n",
    "        - The question mark is the __batch size__ which means that TensorFlow can process batches of data at once\n",
    "        - It's a question mark because you can change the batch size depending on how much data ou want to process at once\n",
    "    - We can also expand the node by clicking layer_1 and expand it\n",
    "        - Inside the node are actions being done insight of this specific layer of the neural network\n",
    "5. Tracing the path of data through the graph\n",
    "    - Click the output_layer and click on 'Trace inputs' on the left side\n",
    "        - This highlights the path data flows through to generate a prediction from the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# Set run name for log folder\n",
    "RUN_NAME = 'run_1_with_50_nodes'\n",
    "RUN_NAME_2 = 'run_2_with_5_nodes'\n",
    "\n",
    "# Get data\n",
    "train = pd.read_csv('Data/sales_data_training_scaled.csv')\n",
    "test = pd.read_csv('Data/sales_data_test_scaled.csv')\n",
    "\n",
    "# Split train data into X and y input arrays\n",
    "X = train.drop('total earnings', axis=1).values\n",
    "y = train[['total earnings']].values\n",
    "\n",
    "# Create a new sequential nn (keras sequential API)\n",
    "model = Sequential()\n",
    "\n",
    "# Add in dense layers and input nodes\n",
    "model.add(Dense(50, input_dim=9, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2')) # second layer\n",
    "model.add(Dense(50, activation='relu', name='layer_3')) # third layer\n",
    "model.add(Dense(1, activation='linear', name='output_layer')) # output layer, predicts 1 value\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimize='adam')\n",
    "\n",
    "# Create a TensorBoard logger object\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),  # pass it a log file parameter; logs = folder name\n",
    "    write_graph=True,                    # log the structure of the model\n",
    "    histogram_freq=5                     # statistics on layers; passes through the training data it will log statistics\n",
    ")\n",
    "\n",
    "# Train the model (tell model to use logger)\n",
    "model.fit(X,                 # training features\n",
    "          y,                 # expected output\n",
    "          epochs=50,         # training passes (epoch)\n",
    "          shuffle=True,      # works best when randomly shuffled\n",
    "          verbose=2,         # print more detailed info\n",
    "          callbacks=[logger] # a list of functions we want Keras to call in an array; can have multiple functions\n",
    ")\n",
    "\n",
    "# Split test data into X and y input arrays\n",
    "X_test = test.drop('total_earnings', axis=1).values\n",
    "y_test = test[['total_earnings']].values\n",
    "\n",
    "# Measure the error rate of the testing data\n",
    "test_error_rate = model.evaluate(X_test, y_test, verbose=0) # pass in verbose 0\n",
    "\n",
    "# Print error rate\n",
    "print('The mean squared error (MSE) for the test data set is: {}'.format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Trained Keras Model in Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you want to be able to scale it up in production to serve lots of users. What should you do? Since we're using Keras with TensorFlow backend, we can export our Keras model as a TensorFlow model and once we have a TensorFlow model we can upload that to Google Cloud ML service. Using the Google Cloud ML service, we can support as many users as we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting TensorFlow model\n",
    "\n",
    "Add this code after building the model from above and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SavedModelBuilder object\n",
    "model_builder = tf.saved_model.builder.SavedModelBuilder('exported_model') # pass in a folder name to save in\n",
    "\n",
    "# Declare model inputs and outputs \n",
    "# Keras makes it easy as it keeps track of i/o of the model so we just need to pass out the TensorFlow\n",
    "\n",
    "# Pass in model.input as the name of the input for Tf to use\n",
    "inputs = {\n",
    "    'input': tf.saved_model.utils.build_tensor_info(model.input) # name of the input\n",
    "}\n",
    "\n",
    "# Pass in the model_output\n",
    "outputs = {\n",
    "    'earnings': tf.saved_models.utils.build_tensor_info(model.output)\n",
    "}\n",
    "\n",
    "# Create a Tf signature def: a function declaration in the programming language\n",
    "# Tf looks for this to know how to run the prediction function of our model\n",
    "signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "\n",
    "# Save both the structure of our model and current weights\n",
    "# Pass in the reference to the current Keras session and assign the model a special\n",
    "#    tag to make sure Tf knows this model is meant for serving users\n",
    "# Pass in the signature def we just created\n",
    "model_builder.add_meta_graph_and_varibles(\n",
    "    K.get_session(),\n",
    "    tags=[tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save (see newly created folder)\n",
    "model_builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "I created the new folder called exported_model. \n",
    "\n",
    "1. First, we have the saved_model, that pb file. This file contains the structure of our model in Google's protobuff format. \n",
    "\n",
    "2. There's also a variable sub-folder. This contains a checkpoint of the train weights from our neural network. \n",
    "\n",
    "Alright, this model's now ready to uploaded to the Google Cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
