{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data\n",
    "\n",
    "The first step of training a machine learning algorithm is loading the training data. \n",
    "- Preload data into memory\n",
    "    - The simplest method is to preload all your data into memory and pass it to TensorFlow as a single array\n",
    "    - Simply __read your data file into an array__ to TensorFlow (size up to computer's available memory)\n",
    "    - As long as the data ends up in a multidimensional array then you're good\n",
    "        - Using pandas and preprocessing the data\n",
    "    \n",
    "    \n",
    "- Feed data step by step\n",
    "    - Write code that feeds your training data step-by-step into TensorFlow as TensorFlow requests it \n",
    "    - TensorFlow calls the data loader function whenever it needs the next chunk of data which gives you more control\n",
    "    - Easier to process large datasets since it loads one chunk at a time\n",
    "    - Have to write all of the code yourself\n",
    "    \n",
    "    \n",
    "- Set up a custom data pipeline\n",
    "    - This is the best option when you are working with enormous datasets like millions of images. A data pipeline allows TensorFlow to manage loading data into memory itself as it needs it\n",
    "    - Data pipeline only loads data into memory in small chunks which means that it can work with large datasets\n",
    "    - Requires writing TensorFlow-specific code\n",
    "    - Big advantage of building a data pipeline is that you can take advantage of __parallel processing__ across multiple CPUs\n",
    "    - Can have several threads running at the same time to load and preprocess data\n",
    "    - Training process doesn't have to stop and wait while the next chunk of data is loaded for the next training pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define model parameters and create the layers\n",
    "\n",
    "Our neural network should accept nine floating point numbers as the input for making predictions. But each time we want a new prediction the specific values we pass in will be different. So we can use a placeholder node to represent that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "learning_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Define input layer (new variable scope)\n",
    "# \"None\" tells TensorFlow our neural network can mix up batches of any size and number_of_inputs tells it to \n",
    "#  expect nine values for each record in the batch\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "    \n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable('weights1', shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases1', shape=[layer_1_nodes], initializer=tf.zeros_initializer)\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases) # matrix multiplication and a standard rectified linear unit \n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable('weights2', shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases2', shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "    \n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable('weights3', shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases3', shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "    \n",
    "# Output layer   \n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable('weights4', shape=[layer_3_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases4', shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "    \n",
    "# Define the cost function\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "    \n",
    "# Define the optimizer function (train & optimize)\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "'''\n",
    "    Training Session\n",
    "    \n",
    "    For session.run, the first command we always run is the built in command to tell TensorFlow to initialize \n",
    "    all variables in our graph to their default values. The command that's called tf.global_variables_initializer. \n",
    "    Now that all the variables in our graph are initialized, we're ready to create our training loop. \n",
    "'''\n",
    "\n",
    "# Initialize a training session after defining the model\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Run the global variable initializer to initialize all variables/layers in the nn\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run the optimizer\n",
    "    for epoch in range(2):\n",
    "        \n",
    "        # Add training data\n",
    "        session.run('optimizer', feed_dict={X: X_scaled_training, Y: Y_scaled_training}) # Pass in operation \n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0:\n",
    "            training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "            testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "            print(epoch, training_cost, testing_cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
