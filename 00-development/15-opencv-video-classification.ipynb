{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-87d6a2b01fb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#----------- blob snippet -------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Create blob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# change img to frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Set Input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.0.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Create a video capture object and read in the input file to check to see that it can open the file\n",
    "cap = cv2.VideoCapture('Videos/shore.mov')\n",
    "\n",
    "# Read the synset classes file and strip off any characters\n",
    "all_rows = open('synset_words.txt').read().strip().split('\\n')\n",
    "\n",
    "# Grab the different descriptions and not the id; we can use a list comprehension. \n",
    "# Find classes; looking for a space, and we don't want to include the id for r in all_rows; +1 for text after space\n",
    "classes = [r[r.find(' ') + 1:] for r in all_rows]\n",
    "\n",
    "# Read the caffe file\n",
    "net = cv2.dnn.readNetFromCaffe('CaffeModel/bvlc_googlenet.prototxt', 'CaffeModel/bvlc_googlenet.caffemodel')\n",
    "\n",
    "# Make sure we can open the video file\n",
    "if cap.isOpened() == False:\n",
    "    print('Cannot open file or video stream')\n",
    "    \n",
    "# Read and display video frames until either the video is completed or the user quits by typing an escape key\n",
    "while True:\n",
    "    \n",
    "    # Call that return and frame and return true to see the frame\n",
    "    ret, frame = cap.read() # returns 2 parameters\n",
    "    \n",
    "    #----------- blob snippet -------------------------------------------------\n",
    "    # Create blob\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (224, 224)) # change img to frame\n",
    "\n",
    "    # Set Input\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Set forward pass to get the predictions for each of the 1,000 classes\n",
    "    output = net.forward()\n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    r=1\n",
    "    for i in np.argsort(output[0])[::-1][:5]:\n",
    "        \n",
    "        # Text being printed on each frame\n",
    "        txt = ' \"%s\" probability \"%.3f\" ' % (classes[i], output[0][i] * 100) # (class, probability)\n",
    "        \n",
    "        # putText(frame, text, location of text, font, size, color, thickness)\n",
    "        cv2.putText(frame, txt, (0, 25+40*r), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "        r+=1\n",
    "        \n",
    "    # If true, show frame\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        # Pass a number greater than zero for videos; OpenCV suggests 25 milliseconds\n",
    "        if cv2.waitKey(25) & 0xFF == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Once we're done, release the video and remove windows        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video is made up of multiple images; so all we're doing is taking each image of frame and checking against the model. So if we head back to the image classification file, we can reuse the blob from the image file, and this time, the image will be called a frame.\n",
    "\n",
    "We can reuse the blob from the image file, and this time, the image will be called a frame. Then, after creating the blob, we can then set it as the input to the network, and we can then perform a forward pass to get the predictions for each of the 1000 classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (OpenCV)",
   "language": "python",
   "name": "ocv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
