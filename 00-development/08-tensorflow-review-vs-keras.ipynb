{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data\n",
    "\n",
    "The first step of training a machine learning algorithm is loading the training data. \n",
    "- Preload data into memory\n",
    "    - The simplest method is to preload all your data into memory and pass it to TensorFlow as a single array\n",
    "    - Simply __read your data file into an array__ to TensorFlow (size up to computer's available memory)\n",
    "    - As long as the data ends up in a multidimensional array then you're good\n",
    "        - Using pandas and preprocessing the data\n",
    "    \n",
    "    \n",
    "- Feed data step by step\n",
    "    - Write code that feeds your training data step-by-step into TensorFlow as TensorFlow requests it \n",
    "    - TensorFlow calls the data loader function whenever it needs the next chunk of data which gives you more control\n",
    "    - Easier to process large datasets since it loads one chunk at a time\n",
    "    - Have to write all of the code yourself\n",
    "    \n",
    "    \n",
    "- Set up a custom data pipeline\n",
    "    - This is the best option when you are working with enormous datasets like millions of images. A data pipeline allows TensorFlow to manage loading data into memory itself as it needs it\n",
    "    - Data pipeline only loads data into memory in small chunks which means that it can work with large datasets\n",
    "    - Requires writing TensorFlow-specific code\n",
    "    - Big advantage of building a data pipeline is that you can take advantage of __parallel processing__ across multiple CPUs\n",
    "    - Can have several threads running at the same time to load and preprocess data\n",
    "    - Training process doesn't have to stop and wait while the next chunk of data is loaded for the next training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Clear session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load the training data\n",
    "training_data_df = pd.read_csv('Data/sales_data_training.csv', dtype=float) # expect float-point numbers\n",
    "\n",
    "# Get X and Y data\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values # call .values to get back the result as an array\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data\n",
    "test_data_df = pd.read_csv('Data/sales_data_test.csv', dtype=float) # expect float-point numbers\n",
    "\n",
    "# Get X and y test data\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values # .values for an array\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "''' \n",
    "  Scale the data (normalizing it to a range)\n",
    "  \n",
    "  The scaler scales the data by multiplying it by a constant number and adding a constant number.\n",
    "'''\n",
    "\n",
    "# Preprocess data\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the data using scaler\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# Scale testing\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# Print shape\n",
    "print(X_scaled_testing.shape)\n",
    "print(Y_scaled_testing.shape)\n",
    "\n",
    "print('Note: Y values were scaled by multiplying by {:.10f} and adding {:4f}'.format(Y_scaler.scale_[0], Y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define model layers, initialize, predict, & save\n",
    "\n",
    "Our neural network should accept nine floating point numbers as the input for making predictions. But each time we want a new prediction the specific values we pass in will be different. So we can use a placeholder node to represent that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "####### Define the model\n",
    "################################################\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "learning_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Define input layer (new variable scope)\n",
    "# \"None\" tells TensorFlow our neural network can mix up batches of any size and number_of_inputs tells it to \n",
    "#  expect nine values for each record in the batch\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "    \n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable('weights1', shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases1', shape=[layer_1_nodes], initializer=tf.zeros_initializer)\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases) # matrix multiplication and a standard rectified linear unit \n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable('weights2', shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases2', shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "    \n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable('weights3', shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases3', shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "    \n",
    "# Output layer   \n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable('weights4', shape=[layer_3_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases4', shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "    \n",
    "# Define the cost function\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "    \n",
    "# Define the optimizer function (train & optimize)\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "'''\n",
    "    Logging Session\n",
    "    \n",
    "    Log our metrics so we are able to view it on TensorBoard. Add scaler objects that will represent the value\n",
    "    we are logging (tf.summary.scaler()) and create a new node for the object. Use tf.summary.merge_all() for multiple,\n",
    "'''    \n",
    "    \n",
    "# Create a summary operation to log the progress of the network (variable to hold the logs)\n",
    "with tf.variable_scope('logging'):\n",
    "    \n",
    "    # Add a scaler object to represent the value we are logging\n",
    "    tf.summary.scaler('current_cost', cost)\n",
    "    \n",
    "    # Execute all summary nodes in the graph withou explicitly lsiting them all\n",
    "    summary = tf.summary.merge_all() # helper function\n",
    "\n",
    "# Create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "################################################\n",
    "####### Initialize Session\n",
    "################################################\n",
    "\n",
    "'''\n",
    "    Training Session\n",
    "    \n",
    "    For session.run, the first command we always run is the built in command to tell TensorFlow to initialize \n",
    "    all variables in our graph to their default values. The command that's called tf.global_variables_initializer. \n",
    "    Now that all the variables in our graph are initialized, we're ready to create our training loop. \n",
    "'''\n",
    "\n",
    "# Initialize a training session after defining the model\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Run the global variable initializer to initialize all variables/layers in the nn\n",
    "    session.run(tf.global_variables_initializer()) # executes commands by calling session.run & pass in global initializer\n",
    "    \n",
    "    # Create log file writers to record the training progress - store train and test in different folders\n",
    "    training_writer = tf.summary.FileWriter('./logs/training', session.graph)\n",
    "    testing_writer = tf.summary.FileWriter('./logs/testing', session.graph)\n",
    "    \n",
    "    # Run the optimizer\n",
    "    for epoch in range(learning_epochs):\n",
    "        \n",
    "        # Add training data\n",
    "        session.run('optimizer', feed_dict={X: X_scaled_training, Y: Y_scaled_training}) # pass in operation \n",
    "        \n",
    "        # Log the progress after every 5 epochs; add in logging variables & cost\n",
    "        if epoch % 5 == 0:\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "            print(epoch, training_cost, testing_cost)\n",
    "            \n",
    "            # Write the current accuracy score by running the 'cost' operation\n",
    "            training_writer.add_summary(training_summary, epoch) # writes cost per epoch to the graph\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "        # Training complete\n",
    "        print('Training is now complete.')\n",
    "        \n",
    "        final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "        final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Yscaled_testing})\n",
    "        \n",
    "        print('Final Training Cost: {}'.format(final_training_cost))\n",
    "        print('Final Testing Cost: {}'.format(final_testing_cost))\n",
    "\n",
    "################################################\n",
    "####### Prediction\n",
    "################################################  \n",
    "    \n",
    "    # Create a new session for predictions; run the 'prediction' operation (output layer)\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "    \n",
    "    # Rescale back to the original units\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "    \n",
    "    # Compare the first value of the real & predicted earnings\n",
    "    real_earnings = test_data_df['total_earnings'].values[0] # first value\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "    \n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))   \n",
    "    \n",
    "    # Save the model\n",
    "    save_path = saver.save(session, 'logs/trained_model.ckpt')\n",
    "    print('Model Saved: {}'.format(save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terminal: tensorboard --logdir=filepath/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Steps\n",
    "\n",
    "1. Get training data from CSV files\n",
    "2. Get X & Y features\n",
    "3. Preprocess the data using the training dataset using - MinMaxScaler()\n",
    "4. Scale the training data using - fit_transform()\n",
    "5. Scale the testing data using - transform()\n",
    "6. Define the model structure\n",
    "    1. The first layer will have 50 nodes\n",
    "    2. The second layer will have 100 nodes\n",
    "    3. The third layer will have 50 nodes\n",
    "7. Define cost and optimizer functions\n",
    "8. Define model logging variables for TensorBoard\n",
    "9. Initialize a training session\n",
    "10. Train the model\n",
    "11. Predict using new data\n",
    "12. Save the model object\n",
    "13. Open Tensorboard (Optional) - tensorboard --logdir=filepath/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model for Production Deployment\n",
    "### There's several ways we can use this model. \n",
    "\n",
    "1. First, if we want to initialize all the variables to their default values, we call this init operator. \n",
    "2. If we want to generate an output we can pass an input data and then call the output operation. \n",
    "3. If we want to train the network we can call the train operator. If we export this model to a file using the normal way of saving model checkpoint files, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "########## Build Model for Production Deployment\n",
    "############################################################\n",
    "    \n",
    "    # Create a new saved model builder object\n",
    "    model_builder = tf.saved_model_builder.SavedModelBuilder('exported_model')\n",
    "    \n",
    "    '''\n",
    "    First, we have a Python dictionary with a key called inputs. In this dictionary, we'll list each tensor \n",
    "    that needs to be filled in when their model is run. Our model takes in one tensor with nine values as input, \n",
    "    so that means our model will have one input.\n",
    "    '''\n",
    "    \n",
    "    # Our model takes in one tensor with nine values as input that means our mode will have one input\n",
    "    inputs = {\n",
    "        'input': tf.saved_model.utils.build_tensor_info(X)\n",
    "    }\n",
    "    \n",
    "    # Output is a single tensor with one value\n",
    "    outputs = {\n",
    "        'earnings': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    A signature def is sort of like a function or method declaration in the \n",
    "    programming language. We're telling TensorFlow that to run the model it should call a certain \n",
    "    function with certain parameters\n",
    "    '''\n",
    "    \n",
    "    # Define signature def\n",
    "    signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    Use meta graph as the structure of the computational graph and the variables are the \n",
    "    values we set on each nodein the graph. This is telling TensorFlow that we want to export everything\n",
    "    '''\n",
    "    \n",
    "    # Configure model builder on how the model is exported\n",
    "    model_builder.add_meta_graph_and_variables(\n",
    "        session,\n",
    "        tags=[tf.saved_model.tag_constants.SERVING],\n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Save the model builder\n",
    "    model_builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
