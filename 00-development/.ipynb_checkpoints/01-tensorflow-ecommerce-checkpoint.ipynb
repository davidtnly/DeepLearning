{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a TensorFlow Model\n",
    "TensorFlow is a software framework for building and deploying machine learning models. It provides the basic building blocks to design, train, and deploy machine learning models. It can be used for several machine learning algorithms. Mostly it is famous for building deep neural networks such as image recognition, speech recognition and translation, image style transfer, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ecommerce_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new function\n",
    "def get_nulls(df):\n",
    "    \n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(df.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)/len(df),2), columns=['Null Data Pct'])\n",
    "\n",
    "    # Combine dataframes horizontally\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "    # Print\n",
    "    print('There are', len(all_nulls), 'columns with missing values.')\n",
    "    return all_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nulls(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X / y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training\n",
    "y = data['Yearly Amount Spent']\n",
    "X = data.drop(['Yearly Amount Spent'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assign variables for test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(X_train):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Y_training', y_train.shape)\n",
    "print('Shape of Y_testing', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape output variable\n",
    "\n",
    "https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_re = np.reshape(y_train, (-1, 1))\n",
    "y_test_re = np.reshape(y_test, (-1, 1))\n",
    "\n",
    "print(y_train_re.shape)\n",
    "print(y_test_re.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "It is very important to scale down your data to train neural network (range between 0 and 1). It helps in immediate reduction in cost and improvement in model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale to a range from 0 to 1 for neural networks to work well\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale both the training inputs and outputs\n",
    "X_scaled_train = X_scaler.fit_transform(X_train)\n",
    "y_scaled_train = y_scaler.fit_transform(y_train_re)\n",
    "\n",
    "# Scale test data\n",
    "X_scaled_test = X_scaler.fit_transform(X_test)\n",
    "y_scaled_test = y_scaler.fit_transforM(y_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled_test.shape)\n",
    "print(y_scaled_test.shape)\n",
    "\n",
    "print(\"Note: Y values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(y_scaler.scale_[0], y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "Our data set has 4 input features and 1 output so, our input layer will have 4 units and the output layer will have 1 unit. Initially, we will choose 3 hidden layers with 50, 100 and 50 units in the first, second and third layer respectively. Later, we can test out different layer sizes to see what layer size gives us the best accuracy.\n",
    "\n",
    "There are many different types of layers we can use in the neural network but we will select fully connected neural network layers as they are the straight-forward type. In a fully connected neural network, every node in each layer is connected to every node in the following layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 4\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100 \n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input layer (1)\n",
    "\n",
    "The neural network should accept four floating point numbers as the input for making predictions. It is easy for us to define four different variables but consider a case where we have a hundred or thousand input features. \n",
    "\n",
    "TensorFlow made our life easy with placeholder object. We will define our input X with tf.placeholder object with its data type and shape. We will define the shape as (None, number_of_inputs). ‘None’ means that our neural network can mix up batches of any size and ‘number_of_inputs’ tells it to expect four values for each record in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer (first layer)\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hidden layers (1, 2, 3)\n",
    "\n",
    "We will define the hidden layers with the same concept of variable scope and name each layer. Within the scope of each layer, we will define its weights, biases and activation function (A=W.X+b). \n",
    "\n",
    "The weight and bias will be variables instead of a placeholder because we want TensorFlow to remember the value over time. We will define them using tf.getvariable and pass in the name. For each layer, we will assign a different name for the bias, weight and the layer itself. The bias value shall be initialized to zero using tf.zero_initializer function while weight shall be initialized randomly using tf.contrib.layers.xavier_initializer function. \n",
    "\n",
    "The activation function of each layer shall be calculated using tf.matmul and tf.nn.relu functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name='weights1',\n",
    "                              shape=[number_of_inputs, layer_1_nodes],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\",\n",
    "                             shape=[layer_1_nodes],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name='weights2',\n",
    "                             shape=[layer_1_nodes, layer_2_nodes],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name='biases2',\n",
    "                            shape=[layer_2_nodes],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(X, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name=\"weights3\",\n",
    "                              shape=[layer_2_nodes, layer_3_nodes],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\",\n",
    "                             shape=[layer_3_nodes],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define output layer\n",
    "\n",
    "Output layer will be defined in similar to hidden layers but the names of variables will be different. We will use 'prediction' as the name for activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name=\"weights4\",\n",
    "                              shape=[layer_3_nodes, number_of_outputs],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\",\n",
    "                             shape=[number_of_outputs],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cost function\n",
    "\n",
    "Define the cost function for our neural network to measure the accuracy of our model. The cost function is the sum of the square of differences between the predicted and real values. We will use tf.reduce_mean and tf.squared_difference functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "TensorFlow also provides the built-in function for optimizers. In our case, we will use the Adam optimizer which is very powerful and most commonly used standard optimizer. We will call tf.train.AdamOptimizer function with learning rate as the input parameter. Next, we will pass the cost function as a variable to minimize. \n",
    "\n",
    "This single line code tells TensorFlow that whenever we call the optimizer, it should run one iteration of the Adam optimizer in an attempt to make the cost value smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    # For scalar or single value \n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    \n",
    "    # To automatically execute all summary nodes in the graph\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Save a model for future use\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model Training Loop\n",
    "\n",
    "First, we will initialize a session so that we can run TensorFlow operations and then we will run the global variable initializer to initialize all variables and layers of the neural network. We have defined 'training_epoch' as our model parameter which will decide how many times the optimizer will run to train the network. \n",
    "\n",
    "For every epoch, we will feed in the training data and do one step of neural network training. For every 5 epochs, we will log our training progress and print it. After completing all training epochs, we will calculate our final training and testing cost and print them. \n",
    "\n",
    "After training our model, we will make predictions by passing on the X testing data and run the “prediction” operation. The output of prediction will be unscaled by the Y scaler. Finally, we will print the two values (predicted and real values) and saved our model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_train, Y: Y_scaled_train})\n",
    "\n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_train, Y: Y_scaled_train})\n",
    "            testing_cost, testing_summary =  session.run([cost, summary], feed_dict={X: X_scaled_test, Y: Y_scaled_test})\n",
    "            \n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")\n",
    "    \n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_train, Y: Y_scaled_train})\n",
    "    final_testing_cost =  session.run(cost, feed_dict={X: X_scaled_test, Y: Y_scaled_test})\n",
    "    \n",
    "    print('Final Training Cost = {}'.format(final_training_cost))\n",
    "    print('Final Testing Cost = {}'.format(final_testing_cost))\n",
    "\n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediction\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_test})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = y_test[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual yearly amount spent were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted spending of ${}\".format(predicted_earnings))\n",
    "    \n",
    "    save_path = saver.save(session, \"trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Model for Prediction\n",
    "\n",
    "Once the model is trained and saved then we can just load it to predict for the new data set. All steps are same as above except we don’t initialize the variables and run through all the training epochs. We will load the trained model and pass in the input data and run the 'prediction' operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # When loading from a checkpoint, don't initialize the variables!\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Instead, load them from disk:\n",
    "    saver.restore(session, \"trained_model.ckpt\")\n",
    "\n",
    "    print(\"Trained model loaded from disk.\")\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    training_cost = session.run(cost, feed_dict={X: X_scaled_train, Y: Y_scaled_train})\n",
    "    testing_cost = session.run(cost, feed_dict={X: X_scaled_test, Y: Y_scaled_test})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(testing_cost))\n",
    "\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_test})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = y_test[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual yearly amount spend were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted spending of ${}\".format(predicted_earnings))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
