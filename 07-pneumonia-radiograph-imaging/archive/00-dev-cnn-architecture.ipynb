{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator to feed into the network in batch sizes\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.3,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.3,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Set paths and parameters\n",
    "train_gen_path = MAIN_PATH / 'train_gen'\n",
    "test_gen_path = MAIN_PATH / 'test_gen'\n",
    "seed=100\n",
    "\n",
    "# Set generation object path\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    save_to_dir=train_gen_path\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    save_to_dir=test_gen_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Build the neural network\n",
    "def build_model():\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(150, 150, 3), name='input_layer')\n",
    "    \n",
    "    # First convolutional block\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_2')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool1_1')(x) # max pooling operation for spatial data\n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_1')(x)\n",
    "    x = BatchNormalization(name='bn2_1')(x) # stabilizing the learning process; reduce internal covariate shifting\n",
    "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_2')(x)\n",
    "    x = BatchNormalization(name='bn2_2')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool2_1')(x)\n",
    "    \n",
    "    # Third convolutional block\n",
    "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_1')(x)\n",
    "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_2')(x)\n",
    "    x = BatchNormalization(name='bn3_1')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool3_1')(x)   \n",
    "    \n",
    "    # Fourth convolutional block\n",
    "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_1')(x)\n",
    "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_2')(x)\n",
    "    x = BatchNormalization(name='bn4_1')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool4_1')(x)\n",
    "    x = Dropout(rate=0.20, name='dropout4_1')(x) # prevent network from overfitting; probabilistically reduce the network capacity\n",
    "    \n",
    "    # Fifth convolutional block\n",
    "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_1')(x)\n",
    "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_2')(x)\n",
    "    x = BatchNormalization(name='bn5_1')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool5_1')(x)\n",
    "    x = Dropout(rate=0.30, name='dropout5_1')(x) \n",
    "    \n",
    "    # Fully connected block\n",
    "    x = Flatten(name='flatten6_1')(x)\n",
    "    x = Dense(512, activation='relu', name='fc6_1')(x)\n",
    "    x = Dropout(0.7, name='dropout6_1')(x)\n",
    "    x = Dense(128, activation='relu', name='fc6_2')(x)\n",
    "    x = Dropout(0.5, name='dropout6_2')(x)\n",
    "    x = Dense(64, activation='relu', name='fc6_3')(x)\n",
    "    x = Dropout(0.3, name='dropout6_3')(x)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(x) # 1 so fewer parameters and computation are needed\n",
    "    \n",
    "    # Define model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "\n",
    "163/163 [==============================] - 82s 502ms/step - loss: 0.3422 - acc: 0.8388 - val_loss: 1.7437 - val_acc: 0.6480\n",
    "\n",
    "Epoch 2/10\n",
    "\n",
    "163/163 [==============================] - 76s 464ms/step - loss: 0.2893 - acc: 0.8823 - val_loss: 0.5643 - val_acc: 0.6723\n",
    "\n",
    "Epoch 3/10\n",
    "\n",
    "163/163 [==============================] - 76s 465ms/step - loss: 0.2572 - acc: 0.8915 - val_loss: 0.7408 - val_acc: 0.5659\n",
    "\n",
    "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
    "\n",
    "Epoch 4/10\n",
    "\n",
    "163/163 [==============================] - 76s 467ms/step - loss: 0.2121 - acc: 0.9185 - val_loss: 0.4917 - val_acc: 0.8598\n",
    "\n",
    "Epoch 5/10\n",
    "\n",
    "163/163 [==============================] - 76s 465ms/step - loss: 0.1995 - acc: 0.9247 - val_loss: 1.5835 - val_acc: 0.6385\n",
    "\n",
    "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
    "\n",
    "Epoch 6/10\n",
    "\n",
    "163/163 [==============================] - 76s 469ms/step - loss: 0.1881 - acc: 0.9329 - val_loss: 0.3183 - val_acc: 0.9003\n",
    "\n",
    "Epoch 7/10\n",
    "\n",
    "163/163 [==============================] - 79s 487ms/step - loss: 0.1677 - acc: 0.9398 - val_loss: 0.3593 - val_acc: 0.8767\n",
    "\n",
    "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
    "\n",
    "Epoch 8/10\n",
    "\n",
    "163/163 [==============================] - 77s 471ms/step - loss: 0.1685 - acc: 0.9375 - val_loss: 0.3603 - val_acc: 0.8885\n",
    "\n",
    "Epoch 9/10\n",
    "\n",
    "163/163 [==============================] - 77s 471ms/step - loss: 0.1590 - acc: 0.9419 - val_loss: 0.5028 - val_acc: 0.8412\n",
    "\n",
    "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
    "\n",
    "Epoch 10/10\n",
    "\n",
    "163/163 [==============================] - 76s 469ms/step - loss: 0.1594 - acc: 0.9440 - val_loss: 0.3789 - val_acc: 0.8733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the neural network\n",
    "# def build_model():\n",
    "    \n",
    "#     # Input layert\n",
    "#     inputs = Input(shape=(TARGET_SIZE, TARGET_SIZE, 3), name ='input_layer')\n",
    "    \n",
    "#     # First convolutional block\n",
    "#     x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "#     x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_2')(x)\n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool1_1')(x) # max pooling operation for spatial data\n",
    "    \n",
    "#     # Second convolutional block\n",
    "#     x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_1')(inputs)\n",
    "#     x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_2')(x)\n",
    "#     x = BatchNormalization(name='bn2_1')(x) # stabilizing the learning process; reduce internal covariate shifting\n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool2_1')(x)\n",
    "    \n",
    "#     # Third convolutional block\n",
    "#     x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_1')(x)\n",
    "#     x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_2')(x)\n",
    "#     x = BatchNormalization(name='bn3_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool3_1')(x)   \n",
    "    \n",
    "#     # Fourth convolutional block\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_1')(x)\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_2')(x)\n",
    "#     x = BatchNormalization(name='bn4_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool4_1')(x)\n",
    "#     x = Dropout(rate=0.20, name='dropout4_1')(x) # prevent network from overfitting; probabilistically reduce the network capacity\n",
    "    \n",
    "#     # Fifth convolutional block\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_1')(x)\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_2')(x)\n",
    "#     x = BatchNormalization(name='bn5_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool5_1')(x)\n",
    "#     x = Dropout(rate=0.20, name='dropout5_1')(x) \n",
    "    \n",
    "#     # Fully connected block\n",
    "#     x = Flatten(name='flatten6_1')(x)\n",
    "#     x = Dropout(rate=0.5, name='dropout6_1')(x)\n",
    "#     x = Dense(512, activation='relu', name='fc6_1')(x)\n",
    "\n",
    "# #     x = Dropout(rate=0.7, name='dropout6_2')(x)\n",
    "# #     x = Dense(128, activation='relu', name='fc6_2')(x)\n",
    "# #     x = Dropout(rate=0.5, name='dropout6_3')(x)\n",
    "# #     x = Dense(64, activation='relu', name='fc6_3')(x)\n",
    "# #     x = Dropout(rate=0.3, name='dropout6_4')(x)\n",
    "    \n",
    "#     # Output\n",
    "#     output = Dense(units=1, activation='sigmoid', name='output')(x) # 1 so fewer parameters and computation are needed\n",
    "    \n",
    "#     # Define model\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the neural network\n",
    "# def build_model():\n",
    "    \n",
    "#     # Input layert\n",
    "#     inputs = Input(shape=(TARGET_SIZE, TARGET_SIZE, 3), name ='input_layer')\n",
    "    \n",
    "#     # First convolutional block\n",
    "#     x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "#     x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_2')(x)\n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool1_1')(x) # max pooling operation for spatial data\n",
    "    \n",
    "#     # Second convolutional block\n",
    "#     x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_1')(x)\n",
    "#     x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_2')(x)\n",
    "#     x = BatchNormalization(name='bn2_1')(x) # stabilizing the learning process; reduce internal covariate shifting\n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool2_1')(x)\n",
    "    \n",
    "#     # Third convolutional block\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_1')(x)\n",
    "#     x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_2')(x)\n",
    "#     x = BatchNormalization(name='bn3_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool3_1')(x)   \n",
    "    \n",
    "#     # Fourth convolutional block\n",
    "#     x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_1')(x)\n",
    "#     x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', name='conv4_2')(x)\n",
    "#     x = BatchNormalization(name='bn4_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool4_1')(x)\n",
    "#     x = Dropout(rate=0.20, name='dropout4_1')(x) # prevent network from overfitting; probabilistically reduce the network capacity\n",
    "    \n",
    "#     # Fifth convolutional block\n",
    "#     x = SeparableConv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_1')(x)\n",
    "#     x = SeparableConv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same', name='conv5_2')(x)\n",
    "#     x = BatchNormalization(name='bn5_1')(x) \n",
    "#     x = MaxPool2D(pool_size=(2, 2), name='pool5_1')(x)\n",
    "#     x = Dropout(rate=0.20, name='dropout5_1')(x) \n",
    "    \n",
    "#     # Fully connected block\n",
    "#     x = Flatten(name='flatten6_1')(x)\n",
    "#     x = Dense(512, activation='relu', name='fc6_1')(x)\n",
    "#     x = Dropout(rate=0.5, name='dropout6_1')(x)\n",
    "#     x = Dense(256, activation='relu', name='fc6_2')(x)\n",
    "#     x = Dropout(rate=0.5, name='dropout6_2')(x)\n",
    "# #     x = Dense(64, activation='relu', name='fc6_3')(x)\n",
    "    \n",
    "#     # Output\n",
    "#     output = Dense(units=1, activation='sigmoid', name='output')(x) # 1 so fewer parameters and computation are needed\n",
    "    \n",
    "#     # Define model\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
