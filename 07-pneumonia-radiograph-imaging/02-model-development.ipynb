{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The model here is a multi-layer architecture consisting of alternating convolutions and nonlinearities. These layers are followed by fully connected layers leading into a sigmoid. The model follows the architecture similar to VGG16 by K. Simonyan and A. Zisserman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning\n",
    "\n",
    "- Diagnostics\n",
    "- Weight Initialization\n",
    "- Learning Rate\n",
    "- Activation Functions\n",
    "- Network Topology\n",
    "- Batches and Epochs\n",
    "- Regularization\n",
    "- Optimization and Loss\n",
    "- Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABICAYAAADI6S+jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAACEklEQVR4nO3aMWpUYRSG4XMdISkUhSQgKChCOjunt7SycQc2swo7FyBYCLoBF6M2Iti4AhPsRBDk2NgY1DAwv9/M9Xm6udziO81bXGbq7gLg37uQHgDwvxJggBABBggRYIAQAQYIEWCAkIvnvTBN06qqVlVVe4v9u9eu3Bg+KuXk0n56wlCHiy/pCcNc/XySnjDUx95LTxiqry/SE4b6+uHTaXcfnX0+rfM/4FsHx/34/rONDtsmL+4dpycM9ejy6/SEYR6+ep6eMNSD77fTE4b69uQgPWGod8unb7t7efa5TxAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAydfffX5imVVWtfv68U1XvR48KOqyq0/SIQeZ8W5X7dt3c77vZ3UdnH54b4F9enqY33b3c6KwtMuf75nxblft23dzv+xOfIABCBBggZN0AvxyyYnvM+b4531blvl039/t+a61vwABsjk8QACECDBAiwAAhAgwQIsAAIT8AsnxTG0hUjfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import imgaug as aug\n",
    "import imgaug.augmenters as imaug\n",
    "\n",
    "# Toolbox\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Deep learning libraries\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, ReLU, Activation\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import cv2\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Preset data display\n",
    "pd.options.display.max_seq_items = 1000\n",
    "pd.options.display.max_rows =1000\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# Set notebook colors and palette\n",
    "flatui = ['#9b59b6', '#3498db', '#95a5a6', '#e74c3c', '#34495e', '#2ecc71']\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette(flatui))\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes(palette='deep')\n",
    "# Favorite code to use: #34995e\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Versions:\n",
      "OpenCV Version: 4.0.1\n",
      "TensorFlow Version: 1.13.1\n",
      "TensorFlow Keras Version: 2.2.4-tf\n",
      "\n",
      "TensorFlow-GPU is available\n",
      "TensorFlow CUDA: True\n",
      "Tensorflow GPU Device Currently Activated: /device:GPU:0\n",
      "\n",
      "Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print('Environment Versions:')\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(\"TensorFlow-GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
    "print(f\"TensorFlow CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"Tensorflow GPU Device Currently Activated: {tf.test.gpu_device_name()}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "\n",
    "Add these into a python script and import them if we are not using Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_bar_labels():\n",
    "    '''\n",
    "    Function used to label the relative frequency on top of each bars\n",
    "    '''\n",
    "    # Set font size\n",
    "    fs=15\n",
    "    \n",
    "    # Set plot label and ticks\n",
    "    plt.ylabel('Relative Frequency (%)', fontsize=fs)\n",
    "    plt.xticks(rotation=0, fontsize=fs)\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Set individual bar labels in proportional scale\n",
    "    for x in ax1.patches:\n",
    "        ax1.annotate(str(x.get_height()) + '%', \n",
    "        (x.get_x() + x.get_width()/2., x.get_height()), ha='center', va='center', xytext=(0, 7), \n",
    "        textcoords='offset points', fontsize=fs, color='black')\n",
    "\n",
    "def freq_table(var):\n",
    "    '''\n",
    "    Define plot global variables\n",
    "    Create a function that will populate a frequency table (%)\n",
    "    Get counts per feature then get the percentage over the total counts\n",
    "    '''\n",
    "    global ax, ax1\n",
    "    \n",
    "    # Get Values and pct and combine it into a dataframe\n",
    "    count_freq = var.value_counts()\n",
    "    pct_freq = round(var.value_counts(normalize=True)*100, 2)\n",
    "    \n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Count': count_freq, 'Percentage': pct_freq})\n",
    "    \n",
    "    # Print variable name\n",
    "    print('Frequency of', var.name, ':')\n",
    "    display(df)\n",
    "    \n",
    "    # Create plot\n",
    "    ax1 = pct_freq.plot.bar(title='Percentage of {}'.format(var.name), figsize=(12,8))\n",
    "    ax1.title.set_size(15)\n",
    "    pct_bar_labels()\n",
    "    plt.show()\n",
    "    \n",
    "# Define a null function\n",
    "def get_nulls(df):\n",
    "    \n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(df.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)/len(df),2), columns=['Null Data Pct'])\n",
    "\n",
    "    # Combine dataframes horizontally\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "    # Print\n",
    "    print('There are', len(all_nulls), 'columns with missing values.')\n",
    "    return all_nulls\n",
    "\n",
    "# Define plot_nulls function\n",
    "def plot_nulls(train):\n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(train.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(train.isnull().sum().sort_values(ascending=False)/len(train),2)*100, columns=['Null Data %'])\n",
    "\n",
    "    # Combine horizontally (axis=1) into a dataframe with column names (keys=[]) then to a data frame\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data %']>0]\n",
    "\n",
    "    # Create figure space\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Create plot\n",
    "    sns.barplot(x=all_nulls.index,\n",
    "                y='Null Data %',\n",
    "                data=all_nulls)\n",
    "\n",
    "    # Set plot features\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.xlabel('Features', fontsize=15)\n",
    "    plt.ylabel('Percent of Missing Values', fontsize=15)\n",
    "    plt.title('Percent of Missing Data by Features', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\David Ly\\\\Documents\\\\Programming\\\\DeepLearning\\\\07-pneunomia-radiograph-imaging\\\\images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ef5846ae76c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetlogin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\Documents\\\\Programming\\\\DeepLearning\\\\07-pneunomia-radiograph-imaging\\\\images'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\David Ly\\\\Documents\\\\Programming\\\\DeepLearning\\\\07-pneunomia-radiograph-imaging\\\\images'"
     ]
    }
   ],
   "source": [
    "# Set directory\n",
    "PATH = 'C:\\\\Users\\\\' + os.getlogin() + '\\\\Documents\\\\Programming\\\\DeepLearning\\\\07-pneumonia-radiograph-imaging\\\\images'\n",
    "os.chdir(PATH)\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train / test / val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Path\n",
    "MAIN_PATH = Path(PATH)\n",
    "\n",
    "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
    "train_dir = MAIN_PATH / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = MAIN_PATH / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = MAIN_PATH / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "train_cases_dir = train_dir / 'train'\n",
    "val_cases_dir = val_dir / 'val'\n",
    "test_cases_dir = test_dir / 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal vs. pneunomia folders\n",
    "normal_cases_dir = train_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
    "\n",
    "# List of all training images\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# Create an empty list to store training data\n",
    "train_data = []\n",
    "\n",
    "# Loop through normal cases and label = 0 \n",
    "for img in normal_cases:\n",
    "    train_data.append((img, 0))\n",
    "\n",
    "# Loop through pneumonia cases and label = 1\n",
    "for img in pneumonia_cases:\n",
    "    train_data.append((img, 1))\n",
    "\n",
    "# Create a dataframe\n",
    "train_data = pd.DataFrame(train_data, columns=['image', 'label'], index=None)\n",
    "\n",
    "# Shuffle\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to testing images\n",
    "normal_cases_dir = test_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
    "\n",
    "# List of all images for training\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# Create empty list to store the training data\n",
    "test_data = []\n",
    "\n",
    "# Loop through normal images and label = 0\n",
    "for img in normal_cases:\n",
    "    test_data.append((img, 0))\n",
    "    \n",
    "# Loop through pneumonia images and label = 1\n",
    "for img in pneumonia_cases:\n",
    "    test_data.append((img, 1))\n",
    "    \n",
    "# Create a dataframe\n",
    "test_data = pd.DataFrame(test_data, columns=['image', 'label'], index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to testing images\n",
    "normal_cases_dir = val_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
    "\n",
    "# List of all images for training\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# Create empty list to store the training data\n",
    "val_data = []\n",
    "\n",
    "# Loop through normal images and label = 0\n",
    "for img in normal_cases:\n",
    "    val_data.append((img, 0))\n",
    "    \n",
    "# Loop through pneumonia images and label = 1\n",
    "for img in pneumonia_cases:\n",
    "    val_data.append((img, 1))\n",
    "    \n",
    "# Create a dataframe\n",
    "val_data = pd.DataFrame(val_data, columns=['image', 'label'], index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show samples of Pneunomia vs. Normal X-ray scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few samples from each dataset\n",
    "normal_samples = (train_data[train_data['label']==0]['image'].iloc[:3]).tolist()\n",
    "pneumonia_samples = (train_data[train_data['label']==1]['image'].iloc[:3]).tolist()\n",
    "\n",
    "# Combine the data together\n",
    "combined_samples = normal_samples + pneumonia_samples\n",
    "\n",
    "# Set figure space\n",
    "f, ax = plt.subplots(2, 3, figsize=(30,15))\n",
    "\n",
    "# Loop through images\n",
    "for i in range(6):\n",
    "    \n",
    "    # Read the image\n",
    "    img = imread(combined_samples[i])\n",
    "    \n",
    "    # Show image (interesting way to plot)\n",
    "    ax[i//3, i%3].imshow(img, cmap='gray')\n",
    "    ax[i//3, i%3].set_aspect('auto') # fixes alignment\n",
    "    \n",
    "    # Set title\n",
    "    if i<3:\n",
    "        ax[i//3, i%3].set_title('Normal', fontsize=15)\n",
    "    else:\n",
    "        ax[i//3, i%3].set_title('Pneumonia', fontsize=15)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freq_table(train_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label images are unbalanced by 25% so we have to decide if we need more data to train (acquire new images, image augmentations) or downsize the dataset where it's balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freq_table(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the validation data is balanced compared to the training and testing data. The con of this is that the dataset for validation is really small so we have to take that into account if we want to use this dataset or not or combine it with the testing data or use other methods to generate a larger sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating images\n",
    "\n",
    "Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.\n",
    "\n",
    "Keras allows for image augmentation. This is where we generate batches of image data with real time data augmentation. The data will be looped over in batches indefinitely. I also tested it out in one of my development notebooks [here](https://github.com/davidtnly/DeepLearning/blob/master/00-development/12-image-augment.ipynb).\n",
    "\n",
    "[TensorFlow Keras ImageDataGenerator Doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator from Keras\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create paths\n",
    "train_gen_path = MAIN_PATH / 'train_gen'\n",
    "test_gen_path = MAIN_PATH / 'test_gen'\n",
    "\n",
    "# Set generation object path\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "    class_mode='binary',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    save_to_dir=train_gen_path\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "    class_mode='binary',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    save_to_dir=test_gen_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store arrays and labels\n",
    "test_data_arrays = []\n",
    "test_labels = []\n",
    "\n",
    "# Loop through the test data to create an array for each picture\n",
    "for num in range(test_data.image.count()):\n",
    "    for img in test_data:\n",
    "        # Read the image and resize the image to (150, 150)\n",
    "        img = plt.imread(test_data.iloc[num].image) # image\n",
    "        img = cv2.resize(img, (TARGET_SIZE, TARGET_SIZE))\n",
    "        # 3D image and scale\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        label = test_data.iloc[num].label # label\n",
    "        \n",
    "        # Append to the list\n",
    "        test_data_arrays.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert to arrays\n",
    "test_data_arrays = np.array(test_data_arrays)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky version of a Rectified Linear Unit.\n",
    "\n",
    "It allows a small gradient when the unit is not active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "def build_model():\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(TARGET_SIZE, TARGET_SIZE, 3), name ='input_layer')\n",
    "    \n",
    "    # First convolutional block\n",
    "    x = SeparableConv2D(filters=16, kernel_size=(3, 3), padding='same', name='conv1_1')(inputs)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = SeparableConv2D(filters=16, kernel_size=(3, 3), padding='same', name='conv1_2')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool1_1')(x) # max pooling operation for spatial data\n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='same', name='conv2_1')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)    \n",
    "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='same', name='conv2_2')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)    \n",
    "    x = BatchNormalization(name='bn2_1')(x) # stabilizing the learning process; reduce internal covariate shifting\n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool2_1')(x)\n",
    "    \n",
    "    # Third convolutional block\n",
    "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv3_1')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)       \n",
    "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv3_2')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)    \n",
    "    x = BatchNormalization(name='bn3_1')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool3_1')(x)   \n",
    "    \n",
    "    # Fourth convolutional block\n",
    "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same', name='conv4_1')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)     \n",
    "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same', name='conv4_2')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)    \n",
    "    x = BatchNormalization(name='bn4_1')(x)     \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool4_1')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout4_1')(x) # prevent network from overfitting; probabilistically reduce the network capacity\n",
    "    \n",
    "    # Fifth convolutional block\n",
    "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same', name='conv5_1')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)       \n",
    "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same', name='conv5_2')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)    \n",
    "    x = BatchNormalization(name='bn5_1')(x) \n",
    "    x = MaxPool2D(pool_size=(2, 2), name='pool5_1')(x)\n",
    "    x = Dropout(rate=0.1, name='dropout5_1')(x) \n",
    "    \n",
    "    # Fully connected block\n",
    "    x = Flatten(name='flatten6_1')(x)\n",
    "    x = Dropout(rate=0.3, name='dropout6_1')(x)\n",
    "    x = Dense(512, name='fc6_1')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)   \n",
    "    \n",
    "    # Output\n",
    "    output = Dense(units=1, activation='sigmoid', name='output')(x) # 1 so fewer parameters and computation are needed\n",
    "    \n",
    "    # Define model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "The primary use case is to automatically save checkpoints during and at the end of training. This way you can use a trained model without having to retrain it, or pick-up training where you left ofâ€”in case the training process was interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: once you know a good depth, start training your network with a lower learning rate along with decay\n",
    "# Default: Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "optimizer = Adam(lr=0.001, decay=1e-08) # stochastic optimization; adam updates parameters with an individual learning rate\n",
    "\n",
    "# Creating model and compiling\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save weights\n",
    "# checkpoint_path = '../model_2/best_weights-cp-{epoch:04d}.ckpt'\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "                             #filepath=checkpoint_path,\n",
    "                             #period=5, # save every 5 epochs\n",
    "                             filepath='best_weights_model_16.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True)\n",
    "\n",
    "# Callbacks - view internal states and statistics of the model during training\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.3,\n",
    "                              patience=2,\n",
    "                              verbose=2,\n",
    "                              mode='max') # reduce learning rate when a metric has stopped improving\n",
    "\n",
    "# Stop if model is not improving\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0.1,\n",
    "                           patience=1,\n",
    "                           mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_gen, \n",
    "    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_gen,\n",
    "    validation_steps=test_gen.samples // BATCH_SIZE,\n",
    "    callbacks=[checkpoint, lr_reduce] # early_stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, j in enumerate(['acc', 'loss']):\n",
    "    ax[i].plot(history.history[j])\n",
    "    ax[i].plot(history.history['val_' + j])\n",
    "    ax[i].set_title('Model {}'.format(j))\n",
    "    ax[i].set_xlabel('epochs', fontsize=12)\n",
    "    ax[i].set_ylabel(j, fontsize=12)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "prediction = model.predict(test_data_arrays)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(prediction))*100\n",
    "cm = confusion_matrix(test_labels, np.round(prediction))\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('CM:' )\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = cm[1][0]\n",
    "fp = cm[1][0]\n",
    "fn = cm[0][1]\n",
    "tp = cm[0][0]\n",
    "\n",
    "print('========= Confusion Matrix =========')\n",
    "sns.heatmap(cm, \n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            annot_kws={'size': 20})\n",
    "plt.show()\n",
    "\n",
    "print('\\n========= Evaluation Metrics =========')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {:.3f}%'.format(acc))\n",
    "print('Precision: {:.3f}%'.format(precision))\n",
    "print('Recall: {:.3f}%'.format(recall))\n",
    "print('F1-score: {:.3f}%'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\n========= Training Accuracy =========')\n",
    "print('Train acc: {}'.format(np.round((history.history['acc'][-1])*100, 2)))\n",
    "print('FN: ', fn)\n",
    "print('Total: ', tn+fp+fn+tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
